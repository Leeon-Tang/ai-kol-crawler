# AI KOL 爬虫系统

YouTube AI领域KOL自动发现与分析系统

**⚠️ 免责声明：本项目仅供学习和研究使用，请遵守YouTube服务条款和相关法律法规。**

---

## 项目说明

本项目是一个用于发现和分析YouTube AI领域KOL的自动化工具，帮助研究人员和内容创作者：
- 发现AI相关的优质频道
- 分析频道的内容质量和活跃度
- 追踪AI领域的内容趋势

**使用限制：**
- ✅ 个人学习和研究
- ✅ 数据分析和学术研究
- ✅ 内容创作参考
- ❌ 商业用途（需遵守YouTube API条款）
- ❌ 大规模爬取（请遵守速率限制）
- ❌ 骚扰、垃圾邮件或恶意用途

---

## 🚀 快速开始（3步启动）

### Windows用户

#### 第一步：双击启动脚本

找到项目文件夹中的 `scripts-windows` 文件夹，双击运行：

```
scripts-windows/启动爬虫.bat
```

**首次运行会自动完成以下操作（需要3-5分钟）：**
- ✅ 下载便携式Python（约15MB）
- ✅ 创建虚拟环境
- ✅ 安装所有依赖包
- ✅ 创建配置文件
- ✅ 初始化数据库
- ✅ 启动Web界面

**无需手动安装Python！无需任何配置！**

#### 第二步：等待浏览器自动打开

启动脚本会自动打开浏览器，访问 `http://localhost:8501`

如果浏览器没有自动打开，请手动访问上述地址

#### 第三步：开始使用

在Web界面中：
1. 点击左侧菜单 **🎮 爬虫控制**
2. 选择使用的关键词数量（建议从5个开始）
3. 点击 **▶️ 开始初始发现**
4. 系统会自动跳转到日志页面，实时显示进度

**完成！现在爬虫已经开始工作了！**

---

### Mac/Linux用户

#### 第一步：打开终端

在项目文件夹中右键，选择"在终端中打开"

#### 第二步：运行启动脚本

```bash
chmod +x scripts-mac-linux/启动爬虫.sh
./scripts-mac-linux/启动爬虫.sh
```

**需要Python 3.8+**

如未安装Python：
- Mac: `brew install python3`
- Ubuntu: `sudo apt-get install python3 python3-pip python3-venv`

#### 第三步：等待浏览器自动打开

启动脚本会自动打开浏览器，访问 `http://localhost:8501`

#### 第四步：开始使用

在Web界面中：
1. 点击左侧菜单 **🎮 爬虫控制**
2. 选择使用的关键词数量（建议从5个开始）
3. 点击 **▶️ 开始初始发现**
4. 系统会自动跳转到日志页面，实时显示进度

**完成！现在爬虫已经开始工作了！**

---

## 📖 详细使用指南

### Web界面功能说明

启动后，你会看到以下页面：

#### 1. 📊 数据看板
- 查看已发现的KOL总数
- 查看合格KOL数量
- 查看最近发现的KOL列表
- 实时统计数据

#### 2. 🎮 爬虫控制（重要！）
这是启动爬虫的地方：

**初始发现任务**：
- 通过关键词搜索YouTube，发现新的AI KOL
- 选择使用的关键词数量（1-50个）
- 建议首次使用5个关键词，大约需要5-10分钟
- 点击"开始初始发现"后，系统会自动跳转到日志页面

**扩散发现任务**：
- 从已有KOL的推荐列表中发现新KOL
- 需要先运行"初始发现"任务
- 可以持续扩散，发现更多相关KOL

#### 3. 📁 数据浏览
- 浏览所有已发现的KOL
- 按状态、AI占比、订阅数等筛选
- 导出Excel报告
- 下载CSV数据

#### 4. 📋 日志查看（重要！）
- 实时查看爬虫运行状态
- 查看详细的抓取日志
- 自动刷新功能（每3秒）
- 清空日志功能

#### 5. 🎯 AI规则
配置AI内容识别规则：
- **AI相关度阈值**：只有AI内容占比超过此值的频道才会被标记为合格（建议30-50%）
- **AI关键词库**：配置高/中/低优先级关键词
- **排除规则**：排除课程、学术、新闻等类型的频道

#### 6. ⚙️ 系统设置
配置搜索和导出选项：
- 搜索关键词列表
- 导出文件格式
- 其他系统参数

---

## 🎯 首次使用建议

### 步骤1：检查配置（可选）

配置文件会在首次启动时自动创建，默认配置已经可以直接使用。

如果你想自定义配置，可以：
1. 在Web界面中点击 **🎯 AI规则**
2. 调整AI相关度阈值（建议30-50%）
3. 查看和修改AI关键词库
4. 配置排除规则

### 步骤2：启动第一次爬取

1. 点击 **🎮 爬虫控制**
2. 在"初始发现任务"中，选择 **5个关键词**（首次建议从小规模开始）
3. 点击 **▶️ 开始初始发现**
4. 系统会自动跳转到 **📋 日志查看** 页面

### 步骤3：观察运行过程

在日志页面中，你会看到：
- ✅ 爬虫运行状态（运行中/已停止）
- 📝 实时日志输出
- 🔄 自动刷新（每3秒）

**预计时间**：5个关键词大约需要5-10分钟

### 步骤4：查看结果

爬取完成后：
1. 点击 **📊 数据看板** 查看统计数据
2. 点击 **📁 数据浏览** 查看详细的KOL列表
3. 点击 **📥 导出Excel** 生成报告

### 步骤5：扩大规模（可选）

如果首次爬取效果满意，可以：
1. 返回 **🎮 爬虫控制**
2. 点击 **▶️ 开始扩散发现**（从已有KOL扩散）
3. 或者增加关键词数量，再次运行"初始发现"

---

## 🔧 技术说明

### 数据存储

- **数据库类型**：SQLite（轻量级，无需安装）
- **数据文件位置**：`data/ai_kol_crawler.db`
- **无需Docker**：不需要安装PostgreSQL或Docker

### 日志系统

- **日志文件位置**：`logs/` 目录
- **命名规则**：按日期命名（如 `20260123.log`）
- **查看方式**：Web界面的"日志查看"页面，或直接打开日志文件

### 导出功能

- **导出格式**：Excel (.xlsx) 和 CSV
- **导出位置**：`exports/` 目录
- **文件命名**：`kol_report_YYYYMMDD_HHMMSS.xlsx`

### 配置文件

- **位置**：`config/config.json`
- **自动创建**：首次启动时从 `config.example.json` 自动创建
- **不会上传**：`.gitignore` 已配置，你的自定义配置不会被上传到Git

---

## ❓ 常见问题

### 启动相关

**Q: 双击启动脚本后，窗口一闪而过？**

A: 这通常是因为：
1. 下载Python失败（网络问题）
2. 依赖安装失败
3. 端口被占用

解决方法：
- 右键点击启动脚本，选择"编辑"查看错误信息
- 或者在命令行中运行脚本，查看详细错误

**Q: 浏览器没有自动打开？**

A: 手动打开浏览器，访问 `http://localhost:8501`

如果8501端口被占用，脚本会自动尝试8502-8510端口，查看命令行窗口中的提示。

**Q: 提示"端口被占用"？**

A: 系统会自动尝试8501-8510端口。如果全部被占用：
1. 关闭其他Streamlit应用或占用端口的程序
2. 重启电脑
3. 重新运行启动脚本

### 运行相关

**Q: 爬虫一直显示"运行中"，但没有日志输出？**

A: 
1. 检查网络连接（需要访问YouTube）
2. 刷新日志页面
3. 如果确实卡住，点击"标记为已完成"，然后重新启动

**Q: 爬取速度很慢？**

A: 这是正常的，因为：
1. 每次请求之间有2秒延迟（避免被封禁）
2. 每个频道需要分析10个视频
3. 5个关键词大约需要5-10分钟

如果想加快速度，可以在"AI规则"中调整参数，但可能增加被封禁的风险。

**Q: 找到的KOL数量很少？**

A: 可能的原因：
1. AI相关度阈值设置过高（建议30-50%）
2. 关键词数量太少（建议至少5个）
3. 排除规则过于严格

解决方法：
- 在"AI规则"中降低AI相关度阈值
- 增加关键词数量
- 检查排除规则是否过于严格

### 数据相关

**Q: 如何导出数据？**

A: 
1. 点击 **📁 数据浏览**
2. 点击 **📥 导出Excel** 或 **📥 下载CSV**
3. 文件会保存在 `exports/` 目录

**Q: 数据保存在哪里？**

A: 
- 数据库文件：`data/ai_kol_crawler.db`
- 日志文件：`logs/YYYYMMDD.log`
- 导出文件：`exports/kol_report_*.xlsx`

**Q: 如何备份数据？**

A: 
1. 复制整个 `data` 文件夹
2. 或者运行备份脚本：`scripts-windows/每日备份.bat`

### Windows特有问题

**Q: 下载Python失败？**

A: 如果自动下载失败：
1. 访问 https://www.python.org/downloads/
2. 下载 "Windows embeddable package (64-bit)"
3. 解压到项目的 `python-portable` 目录
4. 重新运行启动脚本

**Q: 提示"无法运行脚本"？**

A: 
1. 确保文件名没有被修改
2. 右键点击脚本，选择"以管理员身份运行"
3. 检查杀毒软件是否拦截

### Mac/Linux特有问题

**Q: Python版本过低？**

A: 确保Python版本 >= 3.8：
```bash
python3 --version
```

如果版本过低：
- Mac: `brew upgrade python3`
- Ubuntu: `sudo apt-get install python3.8`

**Q: 提示"权限被拒绝"？**

A: 
```bash
chmod +x scripts-mac-linux/启动爬虫.sh
```

### 依赖安装问题

**Q: 依赖安装失败？**

A: 手动安装依赖：
```bash
# Windows
venv\Scripts\pip.exe install -r requirements.txt

# Mac/Linux
source venv/bin/activate
pip install -r requirements.txt
```

---

## 数据备份

### 为什么要备份？

- 保护你辛苦爬取的数据
- 防止意外删除或损坏
- 记录每天的爬取进度

### 自动备份（推荐）

**每天运行一次备份脚本：**

Windows:
```bash
双击 scripts-windows/每日备份.bat
```

Mac/Linux:
```bash
chmod +x scripts-mac-linux/每日备份.sh
./scripts-mac-linux/每日备份.sh
```

### 备份内容

1. **当天数据** - 只包含今天新增的KOL和视频
2. **日志文件** - 今天和昨天的日志
3. **完整数据库** - 所有历史数据的完整备份

### 备份位置

```
backups/
├── 20260122/
│   ├── daily_data_20260122.db      # 今天的数据
│   ├── 20260122.log                # 今天的日志
│   └── full_database_backup.db     # 完整备份
├── 20260123/
│   └── ...
```

### 自动清理

备份脚本会自动删除7天前的旧备份，节省空间。

---

## 📝 项目结构

```
ai-kol-crawler/
├── app.py                      # Streamlit Web界面（主界面）
├── main.py                     # 命令行入口（可选）
├── backup_daily.py             # 备份逻辑（Python脚本）
├── requirements.txt            # Python依赖
├── scripts-windows/            # Windows脚本
│   ├── 启动爬虫.bat            # 一键启动（零配置）
│   └── 每日备份.bat            # 数据备份
├── scripts-mac-linux/          # Mac/Linux脚本
│   ├── 启动爬虫.sh             # 一键启动
│   └── 每日备份.sh             # 数据备份
├── automation/                 # 自动化工具（可选）
├── config/                     # 配置文件
│   ├── config.json             # 主配置文件（自动创建）
│   └── config.example.json     # 配置模板
├── core/                       # 核心功能模块
│   ├── scraper.py              # YouTube爬虫
│   ├── searcher.py             # 关键词搜索
│   ├── analyzer.py             # KOL分析
│   ├── expander.py             # 扩散发现
│   └── filter.py               # 过滤筛选
├── storage/                    # 数据库模块
│   ├── database.py             # 数据库连接（SQLite）
│   ├── models.py               # 数据模型
│   └── kol_repository.py       # 数据访问层
├── tasks/                      # 任务模块
│   ├── discovery_task.py       # 初始发现任务
│   ├── expand_task.py          # 扩散发现任务
│   ├── update_task.py          # 更新任务
│   └── export_task.py          # 导出任务
├── utils/                      # 工具模块
│   ├── logger.py               # 日志系统
│   ├── config_loader.py        # 配置加载
│   ├── text_matcher.py         # 关键词匹配
│   └── exclusion_rules.py      # 排除规则
├── data/                       # 数据库文件（自动创建）
│   └── ai_kol_crawler.db       # SQLite数据库
├── logs/                       # 日志文件（自动创建）
│   └── YYYYMMDD.log            # 按日期命名的日志
├── exports/                    # 导出文件（自动创建）
│   └── kol_report_*.xlsx       # 导出的Excel报告
└── backups/                    # 每日备份（自动创建）
    └── YYYYMMDD/               # 按日期分类
        ├── daily_data_YYYYMMDD.db    # 当天数据
        ├── YYYYMMDD.log              # 当天日志
        └── full_database_backup.db   # 完整数据库备份
```

---

## 🔒 数据备份

### 为什么要备份？

- 保护你辛苦爬取的数据
- 防止意外删除或损坏
- 记录每天的爬取进度

### 自动备份（推荐）

**每天运行一次备份脚本：**

Windows:
```bash
双击 scripts-windows/每日备份.bat
```

Mac/Linux:
```bash
chmod +x scripts-mac-linux/每日备份.sh
./scripts-mac-linux/每日备份.sh
```

### 备份内容

1. **当天数据** - 只包含今天新增的KOL和视频
2. **日志文件** - 今天和昨天的日志
3. **完整数据库** - 所有历史数据的完整备份

### 备份位置

```
backups/
├── 20260122/
│   ├── daily_data_20260122.db      # 今天的数据
│   ├── 20260122.log                # 今天的日志
│   └── full_database_backup.db     # 完整备份
├── 20260123/
│   └── ...
```

### 自动清理

备份脚本会自动删除7天前的旧备份，节省空间。

---

## 🛠️ 高级功能（可选）

### 命令行模式

如果你熟悉命令行，也可以使用命令行模式：

```bash
# Windows
venv\Scripts\activate
python main.py

# Mac/Linux
source venv/bin/activate
python main.py
```

命令行模式提供更多控制选项，但不如Web界面直观。

### 自定义配置

配置文件位于 `config/config.json`，你可以手动编辑：

```json
{
  "crawler": {
    "ai_ratio_threshold": 0.3,        // AI内容占比阈值
    "sample_video_count": 10,         // 每个频道分析视频数
    "rate_limit_delay": 2,            // 请求间隔（秒）
    "max_qualified_kols": 1000        // 最大KOL数量
  },
  "keywords": {
    "priority_high": [...],           // 高优先级关键词
    "priority_medium": [...],         // 中优先级关键词
    "priority_low": [...]             // 低优先级关键词
  },
  "exclusion_rules": {
    "course_keywords": [...],         // 排除课程类
    "academic_keywords": [...],       // 排除学术类
    "news_keywords": [...]            // 排除新闻类
  }
}
```

**建议**：通过Web界面的"AI规则"和"系统设置"页面修改配置，更加直观和安全。

---

## 💡 使用技巧

### 1. 从小规模开始

首次使用建议：
- 使用5个关键词
- AI相关度阈值设为30%
- 观察结果质量

### 2. 逐步扩大规模

如果效果满意：
- 增加到10-20个关键词
- 使用扩散发现功能
- 调整AI相关度阈值

### 3. 定期备份数据

建议每天运行一次备份脚本，保护你的数据。

### 4. 查看日志排查问题

如果遇到问题：
1. 查看"日志查看"页面
2. 查看 `logs/` 目录中的日志文件
3. 根据错误信息排查问题

### 5. 导出数据分析

定期导出Excel报告：
- 分析KOL质量
- 筛选合适的合作对象
- 追踪AI领域趋势

---

## 🔍 数据说明

### KOL评估指标

系统会为每个KOL计算以下指标：

- **AI内容占比**：AI相关视频数 / 总分析视频数
- **订阅数**：频道订阅者数量
- **平均观看数**：所有视频的平均观看量
- **平均点赞数**：所有视频的平均点赞量
- **平均评论数**：所有视频的平均评论量
- **互动率**：(点赞数 + 评论数) / 观看数
- **活跃度**：距离最后一次发布视频的天数

### 状态说明

- **qualified（合格）**：AI内容占比达到阈值，可以考虑合作
- **pending（待分析）**：已发现但尚未完成分析
- **rejected（已拒绝）**：AI内容占比不达标或被排除规则过滤

---

## 🤝 贡献与反馈

### 遇到问题？

1. 查看本README的"常见问题"部分
2. 查看日志文件排查错误
3. 通过GitHub Issues反馈问题

### 想要改进？

欢迎提交Pull Request！

---

## 📄 许可证

本项目采用 MIT License 开源协议。

**重要提示：**
- 本软件仅供教育和研究目的使用
- 用户需自行承担使用本软件的责任
- 请遵守YouTube服务条款和相关法律法规
- 作者不对软件的滥用承担任何责任

详见 [LICENSE](LICENSE) 文件。

---

## 📧 联系方式

如有问题或建议，请通过GitHub Issues联系。

---

**再次提醒：请合法合规使用本工具，尊重数据来源平台的规则和他人的权益。**
