# -*- coding: utf-8 -*-
"""
å¤šå¹³å°çˆ¬è™«ç³»ç»Ÿ - Streamlit Webç•Œé¢
æ”¯æŒYouTubeã€GitHubç­‰å¤šä¸ªå¹³å°
"""
import streamlit as st
import pandas as pd
import time
import threading
import queue
from datetime import datetime
import json
import os
import sys
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)
os.chdir(PROJECT_ROOT)

# çŠ¶æ€æ–‡ä»¶å’Œæ—¥å¿—ç›®å½•è·¯å¾„
CRAWLER_STATUS_FILE = os.path.join(PROJECT_ROOT, "data", "crawler_status.txt")
LOG_DIR = os.path.join(PROJECT_ROOT, "logs")

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å¤šå¹³å°çˆ¬è™«ç³»ç»Ÿ",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åŠ è½½è‡ªå®šä¹‰CSS
def load_css():
    css_file = os.path.join(PROJECT_ROOT, "static", "style.css")
    if os.path.exists(css_file):
        with open(css_file, 'r', encoding='utf-8') as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    else:
        st.warning("CSSæ–‡ä»¶æœªæ‰¾åˆ°")

load_css()

# å…¨å±€æ—¥å¿—é˜Ÿåˆ—
log_queue = queue.Queue()
log_list = []

def add_log(message, level="INFO"):
    """æ·»åŠ æ—¥å¿—"""
    from datetime import datetime, timedelta, timezone
    # ä½¿ç”¨åŒ—äº¬æ—¶é—´
    beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
    timestamp = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] [{level}] {message}"
    log_queue.put(log_entry)
    log_list.append(log_entry)
    if len(log_list) > 1000:
        log_list.pop(0)
    
    # å†™å…¥æ–‡ä»¶
    try:
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file = f"{log_dir}/{beijing_time.strftime('%Y%m%d')}.log"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry + "\n")
    except:
        pass

def set_crawler_running(status):
    """è®¾ç½®çˆ¬è™«è¿è¡ŒçŠ¶æ€"""
    try:
        status_dir = os.path.dirname(CRAWLER_STATUS_FILE)
        os.makedirs(status_dir, exist_ok=True)
        
        # å†™å…¥çŠ¶æ€
        status_text = "running" if status else "stopped"
        with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
            f.write(status_text)
        
        # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜
        if hasattr(os, 'sync'):
            os.sync()
        
        # éªŒè¯å†™å…¥
        with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
            written_status = f.read().strip()
            if written_status != status_text:
                raise Exception(f"çŠ¶æ€å†™å…¥éªŒè¯å¤±è´¥: æœŸæœ› {status_text}, å®é™… {written_status}")
        
        add_log(f"çˆ¬è™«çŠ¶æ€å·²æ›´æ–°: {status_text}", "INFO")
        return True
    except Exception as e:
        add_log(f"è®¾ç½®çˆ¬è™«çŠ¶æ€å¤±è´¥: {str(e)}", "ERROR")
        return False

def is_crawler_running():
    """è·å–çˆ¬è™«è¿è¡ŒçŠ¶æ€"""
    try:
        if os.path.exists(CRAWLER_STATUS_FILE):
            with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
                status = f.read().strip()
                return status == "running"
    except:
        pass
    return False

def check_and_fix_crawler_status():
    """æ£€æŸ¥å¹¶ä¿®å¤çˆ¬è™«çŠ¶æ€ï¼ˆå¦‚æœçº¿ç¨‹å·²ç»“æŸä½†çŠ¶æ€è¿˜æ˜¯è¿è¡Œä¸­ï¼‰"""
    try:
        if not is_crawler_running():
            return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„çˆ¬è™«çº¿ç¨‹
        import threading
        active_threads = threading.enumerate()
        crawler_thread_exists = any(
            'run_youtube_crawler_task' in str(t.name) or 
            'run_github_crawler_task' in str(t.name) or
            t.name.startswith('Thread-') and t.is_alive()
            for t in active_threads
        )
        
        # å¦‚æœçŠ¶æ€æ˜¯è¿è¡Œä¸­ï¼Œä½†æ²¡æœ‰æ´»è·ƒçš„çˆ¬è™«çº¿ç¨‹ï¼Œè‡ªåŠ¨ä¿®å¤
        if not crawler_thread_exists:
            add_log("æ£€æµ‹åˆ°çˆ¬è™«çŠ¶æ€å¼‚å¸¸ï¼ˆæ— æ´»è·ƒçº¿ç¨‹ä½†çŠ¶æ€ä¸ºè¿è¡Œä¸­ï¼‰ï¼Œè‡ªåŠ¨ä¿®å¤", "WARNING")
            set_crawler_running(False)
            return True
        
        return False
    except Exception as e:
        add_log(f"æ£€æŸ¥çˆ¬è™«çŠ¶æ€æ—¶å‡ºé”™: {str(e)}", "ERROR")
        return False

def init_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'db' not in st.session_state:
        st.session_state.db = None
    if 'youtube_repository' not in st.session_state:
        st.session_state.youtube_repository = None
    if 'github_repository' not in st.session_state:
        st.session_state.github_repository = None
    if 'github_academic_repository' not in st.session_state:
        st.session_state.github_academic_repository = None
    if 'twitter_repository' not in st.session_state:
        st.session_state.twitter_repository = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = "youtube_dashboard"
    if 'jump_to_logs' not in st.session_state:
        st.session_state.jump_to_logs = False
    if 'auto_refresh_enabled' not in st.session_state:
        st.session_state.auto_refresh_enabled = True

def connect_database():
    """è¿æ¥æ•°æ®åº“"""
    try:
        if st.session_state.db is None:
            from storage.database import Database
            from storage.repositories.youtube_repository import YouTubeRepository
            from storage.repositories.github_repository import GitHubRepository
            from storage.repositories.github_academic_repository import GitHubAcademicRepository
            from storage.repositories.twitter_repository import TwitterRepository
            
            db = Database()
            db.connect()
            
            # æ£€æŸ¥æ•°æ®åº“å®Œæ•´æ€§
            if not db.check_integrity():
                add_log("æ£€æµ‹åˆ°æ•°æ®åº“æŸåï¼Œå°è¯•ä¿®å¤...", "WARNING")
                if db.repair_database():
                    add_log("æ•°æ®åº“ä¿®å¤æˆåŠŸ", "SUCCESS")
                else:
                    add_log("æ•°æ®åº“ä¿®å¤å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥", "ERROR")
                    st.error("æ•°æ®åº“æŸåä¸”æ— æ³•è‡ªåŠ¨ä¿®å¤ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
                    return False
            
            db.init_tables()
            
            # å°è¯•è¿ç§»æ—§æ•°æ®
            try:
                from storage.migrations.migration_v2 import migrate
                migrate()
            except Exception as e:
                add_log(f"æ•°æ®è¿ç§»æ£€æŸ¥: {e}", "WARNING")
            
            st.session_state.db = db
            st.session_state.youtube_repository = YouTubeRepository(db)
            st.session_state.github_repository = GitHubRepository(db)
            st.session_state.github_academic_repository = GitHubAcademicRepository(db)
            st.session_state.twitter_repository = TwitterRepository(db)
            return True
    except Exception as e:
        st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        add_log(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}", "ERROR")
        return False
    return True

def get_statistics(platform='youtube'):
    """è·å–ç»Ÿè®¡æ•°æ®"""
    if platform == 'youtube' and st.session_state.youtube_repository:
        try:
            return st.session_state.youtube_repository.get_statistics()
        except:
            return {'total_kols': 0, 'qualified_kols': 0, 'pending_kols': 0, 'total_videos': 0, 'pending_expansions': 0}
    elif platform == 'github' and st.session_state.github_repository:
        try:
            stats = st.session_state.github_repository.get_statistics()
            # æ·»åŠ å­¦æœ¯äººå£«ç»Ÿè®¡
            if st.session_state.github_academic_repository:
                academic_stats = st.session_state.github_academic_repository.get_statistics()
                stats.update(academic_stats)
            return stats
        except:
            return {'total_developers': 0, 'qualified_developers': 0, 'pending_developers': 0, 'total_repositories': 0,
                   'total_academic_developers': 0, 'qualified_academic_developers': 0, 'pending_academic_developers': 0}
    elif platform == 'twitter' and st.session_state.twitter_repository:
        try:
            return st.session_state.twitter_repository.get_statistics()
        except:
            return {'total_users': 0, 'qualified_users': 0, 'pending_users': 0, 'total_tweets': 0}
    return {}

def clear_logs():
    """æ¸…ç©ºæ—¥å¿—"""
    global log_list
    log_list.clear()
    from datetime import datetime, timedelta, timezone
    beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
    log_file = os.path.join(LOG_DIR, f"{beijing_time.strftime('%Y%m%d')}.log")
    if os.path.exists(log_file):
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write("")
            return True
        except:
            return False
    return True

def run_youtube_crawler_task(task_type, repository, **kwargs):
    """è¿è¡ŒYouTubeçˆ¬è™«ä»»åŠ¡"""
    task = None
    try:
        from platforms.youtube.scraper import YouTubeScraper
        from platforms.youtube.searcher import KeywordSearcher
        from platforms.youtube.analyzer import KOLAnalyzer
        from platforms.youtube.expander import KOLExpander
        from platforms.youtube.filter import KOLFilter
        from tasks.youtube.discovery import YouTubeDiscoveryTask
        from tasks.youtube.expand import YouTubeExpandTask
        
        set_crawler_running(True)
        add_log(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_type}", "INFO")
        
        scraper = YouTubeScraper()
        searcher = KeywordSearcher(scraper)
        analyzer = KOLAnalyzer(scraper)
        expander = KOLExpander(scraper)
        filter_obj = KOLFilter(repository)
        
        if task_type == "discovery":
            task = YouTubeDiscoveryTask(searcher, analyzer, filter_obj, repository)
            keyword_limit = kwargs.get('keyword_limit', 30)
            add_log(f"ä½¿ç”¨ {keyword_limit} ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢", "INFO")
            task.run(keyword_limit)
        elif task_type == "expand":
            task = YouTubeExpandTask(expander, analyzer, filter_obj, repository)
            add_log("å¼€å§‹æ‰©æ•£ä»»åŠ¡", "INFO")
            task.run()
        
        add_log(f"ä»»åŠ¡å®Œæˆ: {task_type}", "SUCCESS")
        add_log("æ­£åœ¨æ›´æ–°çˆ¬è™«çŠ¶æ€...", "INFO")
        
    except KeyboardInterrupt:
        add_log("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­", "WARNING")
    except Exception as e:
        add_log(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR")
        add_log(traceback.format_exc(), "ERROR")
    finally:
        # ç¡®ä¿çŠ¶æ€ä¸€å®šä¼šè¢«æ›´æ–°
        try:
            add_log("ä»»åŠ¡ç»“æŸï¼Œæ­£åœ¨åœæ­¢çˆ¬è™«...", "INFO")
            success = set_crawler_running(False)
            if success:
                add_log("çˆ¬è™«å·²åœæ­¢", "SUCCESS")
            else:
                add_log("è­¦å‘Šï¼šçˆ¬è™«çŠ¶æ€æ›´æ–°å¯èƒ½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç‚¹å‡»ã€Œæ ‡è®°ä¸ºå·²å®Œæˆã€", "WARNING")
        except Exception as e:
            add_log(f"åœæ­¢çˆ¬è™«æ—¶å‡ºé”™: {str(e)}", "ERROR")
            # å¼ºåˆ¶å†™å…¥
            try:
                with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                    f.write("stopped")
            except:
                pass

def run_github_crawler_task(task_type, repository, **kwargs):
    """è¿è¡ŒGitHubçˆ¬è™«ä»»åŠ¡"""
    task = None
    try:
        from platforms.github.scraper import GitHubScraper
        from platforms.github.searcher import GitHubSearcher
        from platforms.github.analyzer import GitHubAnalyzer
        from tasks.github.discovery import GitHubDiscoveryTask
        
        set_crawler_running(True)
        add_log(f"å¼€å§‹æ‰§è¡ŒGitHubä»»åŠ¡: {task_type}", "INFO")
        
        scraper = GitHubScraper()
        searcher = GitHubSearcher(scraper, repository)  # ä¼ å…¥repositoryç”¨äºå»é‡
        analyzer = GitHubAnalyzer(scraper)
        
        if task_type == "discovery":
            # è·å–å­¦æœ¯äººå£«ä»“åº“
            academic_repository = kwargs.get('academic_repository')
            task = GitHubDiscoveryTask(searcher, analyzer, repository, academic_repository)
            max_developers = kwargs.get('max_developers', 50)
            task.run(max_developers=max_developers)
        
        add_log(f"GitHubä»»åŠ¡å®Œæˆ: {task_type}", "SUCCESS")
        add_log("æ­£åœ¨æ›´æ–°çˆ¬è™«çŠ¶æ€...", "INFO")
        
    except KeyboardInterrupt:
        add_log("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­", "WARNING")
    except Exception as e:
        add_log(f"GitHubä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR")
        add_log(traceback.format_exc(), "ERROR")
    finally:
        # ç¡®ä¿çŠ¶æ€ä¸€å®šä¼šè¢«æ›´æ–°
        try:
            add_log("ä»»åŠ¡ç»“æŸï¼Œæ­£åœ¨åœæ­¢çˆ¬è™«...", "INFO")
            success = set_crawler_running(False)
            if success:
                add_log("çˆ¬è™«å·²åœæ­¢", "SUCCESS")
            else:
                add_log("è­¦å‘Šï¼šçˆ¬è™«çŠ¶æ€æ›´æ–°å¯èƒ½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç‚¹å‡»ã€Œæ ‡è®°ä¸ºå·²å®Œæˆã€", "WARNING")
        except Exception as e:
            add_log(f"åœæ­¢çˆ¬è™«æ—¶å‡ºé”™: {str(e)}", "ERROR")
            # å¼ºåˆ¶å†™å…¥
            try:
                with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                    f.write("stopped")
            except:
                pass

def run_twitter_crawler_task(task_type, repository, **kwargs):
    """è¿è¡ŒTwitterçˆ¬è™«ä»»åŠ¡"""
    task = None
    try:
        from tasks.twitter.discovery import TwitterDiscoveryTask
        
        set_crawler_running(True)
        add_log(f"å¼€å§‹æ‰§è¡ŒTwitterä»»åŠ¡: {task_type}", "INFO")
        
        task = TwitterDiscoveryTask()
        
        if task_type == "keyword_discovery":
            keyword_count = kwargs.get('keyword_count', 5)
            max_users_per_keyword = kwargs.get('max_users_per_keyword', 10)
            add_log(f"ä½¿ç”¨ {keyword_count} ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢", "INFO")
            
            # ä»é…ç½®æ–‡ä»¶è¯»å–å…³é”®è¯
            from utils.config_loader import load_config
            config = load_config()
            keywords = config.get('twitter', {}).get('search_keywords', [])[:keyword_count]
            
            stats = task.discover_by_keywords(keywords, max_results_per_keyword=max_users_per_keyword)
            add_log(f"å…³é”®è¯å‘ç°å®Œæˆ: å‘ç° {stats['total_discovered']} ä¸ªç”¨æˆ·ï¼Œåˆæ ¼ {stats['qualified']} ä¸ª", "SUCCESS")
            
        elif task_type == "hashtag_discovery":
            hashtag_count = kwargs.get('hashtag_count', 3)
            max_users = kwargs.get('max_users', 20)
            add_log(f"ä½¿ç”¨ {hashtag_count} ä¸ªè¯é¢˜æ ‡ç­¾è¿›è¡Œæœç´¢", "INFO")
            
            # ä»é…ç½®æ–‡ä»¶è¯»å–è¯é¢˜æ ‡ç­¾
            from utils.config_loader import load_config
            config = load_config()
            hashtags = config.get('twitter', {}).get('hashtags', [])[:hashtag_count]
            
            stats = task.discover_by_hashtags(hashtags, max_results=max_users)
            add_log(f"è¯é¢˜å‘ç°å®Œæˆ: å‘ç° {stats['total_discovered']} ä¸ªç”¨æˆ·ï¼Œåˆæ ¼ {stats['qualified']} ä¸ª", "SUCCESS")
        
        add_log(f"Twitterä»»åŠ¡å®Œæˆ: {task_type}", "SUCCESS")
        add_log("æ­£åœ¨æ›´æ–°çˆ¬è™«çŠ¶æ€...", "INFO")
        
    except KeyboardInterrupt:
        add_log("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­", "WARNING")
    except Exception as e:
        add_log(f"Twitterä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR")
        add_log(traceback.format_exc(), "ERROR")
    finally:
        # ç¡®ä¿çŠ¶æ€ä¸€å®šä¼šè¢«æ›´æ–°
        try:
            add_log("ä»»åŠ¡ç»“æŸï¼Œæ­£åœ¨åœæ­¢çˆ¬è™«...", "INFO")
            success = set_crawler_running(False)
            if success:
                add_log("çˆ¬è™«å·²åœæ­¢", "SUCCESS")
            else:
                add_log("è­¦å‘Šï¼šçˆ¬è™«çŠ¶æ€æ›´æ–°å¯èƒ½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç‚¹å‡»ã€Œæ ‡è®°ä¸ºå·²å®Œæˆã€", "WARNING")
        except Exception as e:
            add_log(f"åœæ­¢çˆ¬è™«æ—¶å‡ºé”™: {str(e)}", "ERROR")
            # å¼ºåˆ¶å†™å…¥
            try:
                with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                    f.write("stopped")
            except:
                pass



def render_youtube_dashboard():
    """æ¸²æŸ“YouTubeä»ªè¡¨ç›˜"""
    st.markdown('<div class="main-header">YouTube æ•°æ®æ¦‚è§ˆ</div>', unsafe_allow_html=True)
    
    stats = get_statistics('youtube')
    
    # ç¬¬ä¸€è¡Œï¼šä¸»è¦æŒ‡æ ‡ï¼ˆå¤§å¡ç‰‡ï¼‰
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">æ€»KOLæ•°</div>
            <div class="metric-value">{stats.get('total_kols', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="big-metric-card highlight">
            <div class="metric-label">åˆæ ¼KOL</div>
            <div class="metric-value">{stats.get('qualified_kols', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">å¾…åˆ†æ</div>
            <div class="metric-value">{stats.get('pending_kols', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    # ç¬¬äºŒè¡Œï¼šæ¬¡è¦æŒ‡æ ‡ï¼ˆä¸­ç­‰å¡ç‰‡ï¼‰
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">æ€»è§†é¢‘æ•°</div>
            <div class="metric-value-medium">{stats.get('total_videos', 0):,}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">å¾…æ‰©æ•£é˜Ÿåˆ—</div>
            <div class="metric-value-medium">{stats.get('pending_expansions', 0)}</div>
        </div>
        """, unsafe_allow_html=True)

def render_github_dashboard():
    """æ¸²æŸ“GitHubä»ªè¡¨ç›˜"""
    st.markdown('<div class="main-header">GitHub æ•°æ®æ¦‚è§ˆ</div>', unsafe_allow_html=True)
    
    stats = get_statistics('github')
    
    # ç¬¬ä¸€è¡Œï¼šå•†ä¸šå¼€å‘è€…ç»Ÿè®¡
    st.subheader("ğŸ’¼ å•†ä¸š/ç‹¬ç«‹å¼€å‘è€…")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">æ€»å¼€å‘è€…æ•°</div>
            <div class="metric-value">{stats.get('total_developers', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="big-metric-card highlight">
            <div class="metric-label">åˆæ ¼å¼€å‘è€…</div>
            <div class="metric-value">{stats.get('qualified_developers', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">å¾…åˆ†æ</div>
            <div class="metric-value">{stats.get('pending_developers', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    # ç¬¬äºŒè¡Œï¼šå­¦æœ¯äººå£«ç»Ÿè®¡
    st.subheader("ğŸ“ å­¦æœ¯äººå£«")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">æ€»å­¦æœ¯äººå£«</div>
            <div class="metric-value">{stats.get('total_academic_developers', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="big-metric-card highlight">
            <div class="metric-label">åˆæ ¼å­¦æœ¯äººå£«</div>
            <div class="metric-value">{stats.get('qualified_academic_developers', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">å¾…åˆ†æ</div>
            <div class="metric-value">{stats.get('pending_academic_developers', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    # ç¬¬ä¸‰è¡Œï¼šç»¼åˆç»Ÿè®¡
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">æ€»ä»“åº“æ•°</div>
            <div class="metric-value-medium">{stats.get('total_repositories', 0):,}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        qualified = stats.get('qualified_developers', 0)
        total = max(stats.get('total_developers', 1), 1)
        rate = (qualified / total * 100)
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">å•†ä¸šåˆæ ¼ç‡</div>
            <div class="metric-value-medium">{rate:.1f}%</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        repos = stats.get('total_repositories', 0)
        devs = max(stats.get('total_developers', 1) + stats.get('total_academic_developers', 0), 1)
        avg = repos / devs
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">å¹³å‡ä»“åº“æ•°</div>
            <div class="metric-value-medium">{avg:.1f}</div>
        </div>
        """, unsafe_allow_html=True)

def render_twitter_dashboard():
    """æ¸²æŸ“Twitterä»ªè¡¨ç›˜"""
    st.markdown('<div class="main-header">Twitter/X æ•°æ®æ¦‚è§ˆ</div>', unsafe_allow_html=True)
    
    stats = get_statistics('twitter')
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">æ€»ç”¨æˆ·æ•°</div>
            <div class="metric-value">{stats.get('total_users', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="big-metric-card highlight">
            <div class="metric-label">åˆæ ¼ç”¨æˆ·</div>
            <div class="metric-value">{stats.get('qualified_users', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="big-metric-card">
            <div class="metric-label">å¾…åˆ†æ</div>
            <div class="metric-value">{stats.get('pending_users', 0)}</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">æ€»æ¨æ–‡æ•°</div>
            <div class="metric-value-medium">{stats.get('total_tweets', 0):,}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        ai_tweets = stats.get('ai_tweets', 0)
        total_tweets = max(stats.get('total_tweets', 1), 1)
        ai_rate = (ai_tweets / total_tweets * 100)
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">AIæ¨æ–‡å æ¯”</div>
            <div class="metric-value-medium">{ai_rate:.1f}%</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        avg_score = stats.get('avg_quality_score', 0)
        st.markdown(f"""
        <div class="medium-metric-card">
            <div class="metric-label">å¹³å‡è´¨é‡åˆ†</div>
            <div class="metric-value-medium">{avg_score:.1f}</div>
        </div>
        """, unsafe_allow_html=True)

def render_youtube_crawler():
    """æ¸²æŸ“YouTubeçˆ¬è™«æ§åˆ¶"""
    st.markdown('<div class="main-header">YouTube çˆ¬è™«æ§åˆ¶</div>', unsafe_allow_html=True)
    
    # æ£€æŸ¥å¹¶ä¿®å¤çŠ¶æ€
    check_and_fix_crawler_status()
    
    running = is_crawler_running()
    if running:
        st.warning("çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
        st.info("åˆ‡æ¢åˆ°ã€Œæ—¥å¿—æŸ¥çœ‹ã€é¡µé¢æŸ¥çœ‹å®æ—¶è¿›åº¦")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("æ ‡è®°ä¸ºå·²å®Œæˆ", key="mark_complete", use_container_width=True):
                success = set_crawler_running(False)
                if success:
                    st.success("çŠ¶æ€å·²é‡ç½®")
                    time.sleep(0.5)
                else:
                    st.error("çŠ¶æ€é‡ç½®å¤±è´¥")
                st.rerun()
        
        with col2:
            if st.button("å¼ºåˆ¶é‡ç½®çŠ¶æ€", key="force_reset", use_container_width=True):
                try:
                    # å¼ºåˆ¶å†™å…¥
                    with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                        f.write("stopped")
                    st.success("å·²å¼ºåˆ¶é‡ç½®")
                    time.sleep(0.5)
                    st.rerun()
                except Exception as e:
                    st.error(f"å¼ºåˆ¶é‡ç½®å¤±è´¥: {e}")
    else:
        st.success("çˆ¬è™«ç©ºé—²ï¼Œå¯ä»¥å¯åŠ¨æ–°ä»»åŠ¡")
    
    st.divider()
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("åˆå§‹å‘ç°ä»»åŠ¡")
        st.write("é€šè¿‡å…³é”®è¯æœç´¢YouTubeï¼Œå‘ç°æ–°çš„AI KOL")
        
        keyword_limit = st.slider(
            "ä½¿ç”¨å…³é”®è¯æ•°é‡",
            min_value=1,
            max_value=50,
            value=5,
            step=1,
            help="é€‰æ‹©è¦ä½¿ç”¨çš„å…³é”®è¯æ•°é‡ï¼Œæ•°é‡è¶Šå¤šå‘ç°çš„KOLè¶Šå¤šï¼Œä½†è€—æ—¶ä¹Ÿè¶Šé•¿"
        )
        
        st.info(f"é¢„è®¡æœç´¢ {keyword_limit * 5} ä¸ªé¢‘é“ï¼Œè€—æ—¶çº¦ {keyword_limit * 2} åˆ†é’Ÿ")
        
        if st.button("å¼€å§‹åˆå§‹å‘ç°", disabled=running, key="start_youtube_discovery"):
                if not st.session_state.youtube_repository:
                    st.error("æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•å¯åŠ¨ä»»åŠ¡")
                else:
                    clear_logs()
                    add_log("=" * 60, "INFO")
                    add_log("å¼€å§‹æ–°çš„çˆ¬è™«ä»»åŠ¡ - YouTubeåˆå§‹å‘ç°", "INFO")
                    add_log("=" * 60, "INFO")
                    add_log(f"ç”¨æˆ·å¯åŠ¨åˆå§‹å‘ç°ä»»åŠ¡ï¼Œå…³é”®è¯æ•°é‡: {keyword_limit}", "INFO")
                    
                    thread = threading.Thread(
                        target=run_youtube_crawler_task,
                        args=("discovery", st.session_state.youtube_repository),
                        kwargs={'keyword_limit': keyword_limit}
                    )
                    thread.daemon = True
                    thread.start()
                    set_crawler_running(True)
                    st.session_state.jump_to_logs = True
                    st.session_state.auto_refresh_enabled = True
                    time.sleep(0.5)
                    st.rerun()
    
    with col2:
        st.subheader("æ‰©æ•£å‘ç°ä»»åŠ¡")
        st.write("ä»å·²æœ‰KOLçš„æ¨èåˆ—è¡¨ä¸­å‘ç°æ–°KOL")
        
        stats = get_statistics('youtube')
        st.info(f"å½“å‰å¾…æ‰©æ•£é˜Ÿåˆ—: {stats.get('pending_expansions', 0)} ä¸ªKOL")
        
        if stats.get('pending_expansions', 0) == 0:
            st.warning("æ‰©æ•£é˜Ÿåˆ—ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œåˆå§‹å‘ç°ä»»åŠ¡")
        
        if st.button("å¼€å§‹æ‰©æ•£å‘ç°", disabled=running or stats.get('pending_expansions', 0) == 0, key="start_youtube_expand"):
                if not st.session_state.youtube_repository:
                    st.error("æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•å¯åŠ¨ä»»åŠ¡")
                else:
                    clear_logs()
                    add_log("=" * 60, "INFO")
                    add_log("å¼€å§‹æ–°çš„çˆ¬è™«ä»»åŠ¡ - YouTubeæ‰©æ•£å‘ç°", "INFO")
                    add_log("=" * 60, "INFO")
                    add_log("ç”¨æˆ·å¯åŠ¨æ‰©æ•£å‘ç°ä»»åŠ¡", "INFO")
                    
                    thread = threading.Thread(
                        target=run_youtube_crawler_task,
                        args=("expand", st.session_state.youtube_repository)
                    )
                    thread.daemon = True
                    thread.start()
                    set_crawler_running(True)
                    st.session_state.jump_to_logs = True
                    st.session_state.auto_refresh_enabled = True
                    time.sleep(0.5)
                    st.rerun()
    
    st.divider()
    
    with st.expander("é«˜çº§é…ç½®", expanded=False):
        st.subheader("çˆ¬è™«å‚æ•°è®¾ç½®")
        col1, col2 = st.columns(2)
        with col1:
            ai_threshold = st.slider("AIå†…å®¹å æ¯”é˜ˆå€¼", min_value=0.1, max_value=0.9, value=0.3, step=0.05, format="%.0f%%")
            sample_videos = st.number_input("æ¯ä¸ªé¢‘é“åˆ†æè§†é¢‘æ•°", min_value=5, max_value=50, value=10, step=5)
        with col2:
            rate_limit = st.number_input("è¯·æ±‚é—´éš”(ç§’)", min_value=1, max_value=10, value=2, step=1)
            max_kols = st.number_input("æœ€å¤§KOLæ•°é‡", min_value=100, max_value=10000, value=1000, step=100)
        if st.button("ä¿å­˜é…ç½®"):
            st.success("é…ç½®å·²ä¿å­˜ï¼")

def render_github_crawler():
    """æ¸²æŸ“GitHubçˆ¬è™«æ§åˆ¶"""
    from ui.github import render_crawler
    render_crawler(
        is_crawler_running_func=is_crawler_running,
        check_and_fix_status_func=check_and_fix_crawler_status,
        set_crawler_running_func=set_crawler_running,
        clear_logs_func=clear_logs,
        add_log_func=add_log,
        run_crawler_task_func=run_github_crawler_task,
        session_state=st.session_state,
        crawler_status_file=CRAWLER_STATUS_FILE,
        time_module=time,
        threading_module=threading,
        academic_repository=st.session_state.github_academic_repository
    )

def render_twitter_crawler():
    """æ¸²æŸ“Twitterçˆ¬è™«æ§åˆ¶"""
    from ui.twitter import render_crawler
    render_crawler(
        is_crawler_running_func=is_crawler_running,
        check_and_fix_status_func=check_and_fix_crawler_status,
        set_crawler_running_func=set_crawler_running,
        clear_logs_func=clear_logs,
        add_log_func=add_log,
        run_crawler_task_func=run_twitter_crawler_task,
        session_state=st.session_state,
        crawler_status_file=CRAWLER_STATUS_FILE,
        time_module=time,
        threading_module=threading
    )

def render_data_browser():
    """æ¸²æŸ“ç»Ÿä¸€çš„æ•°æ®æµè§ˆé¡µé¢"""
    st.markdown('<div class="main-header">æ•°æ®æµè§ˆ</div>', unsafe_allow_html=True)
    
    # åˆå§‹åŒ–å¹³å°é€‰æ‹©
    if 'data_browser_platform' not in st.session_state:
        st.session_state.data_browser_platform = "YouTube"
    
    # å¹³å°é€‰æ‹© - ä½¿ç”¨æŒ‰é’®ç»„
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("YouTube", key="btn_yt_data", use_container_width=True, 
                    type="primary" if st.session_state.data_browser_platform == "YouTube" else "secondary"):
            st.session_state.data_browser_platform = "YouTube"
            st.rerun()
    with col2:
        if st.button("GitHub", key="btn_gh_data", use_container_width=True,
                    type="primary" if st.session_state.data_browser_platform == "GitHub" else "secondary"):
            st.session_state.data_browser_platform = "GitHub"
            st.rerun()
    # Twitter æ•°æ®æµè§ˆæš‚æ—¶ç¦ç”¨(åŠŸèƒ½æœªå®Œå–„)
    # with col3:
    #     if st.button("Twitter/X", key="btn_tw_data", use_container_width=True,
    #                 type="primary" if st.session_state.data_browser_platform == "Twitter" else "secondary"):
    #         st.session_state.data_browser_platform = "Twitter"
    #         st.rerun()
    
    st.divider()
    
    if st.session_state.data_browser_platform == "YouTube":
        render_youtube_data_content()
    elif st.session_state.data_browser_platform == "GitHub":
        render_github_data_content()
    # Twitter æ•°æ®å†…å®¹æš‚æ—¶ç¦ç”¨(åŠŸèƒ½æœªå®Œå–„)
    # else:
    #     render_twitter_data_content()

def render_youtube_data_content():
    """æ¸²æŸ“YouTubeæ•°æ®å†…å®¹"""
    if not st.session_state.youtube_repository:
        st.warning("è¯·å…ˆè¿æ¥æ•°æ®åº“")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        status_filter = st.selectbox("çŠ¶æ€ç­›é€‰", ["å…¨éƒ¨", "åˆæ ¼", "å¾…åˆ†æ", "å·²æ‹’ç»"], index=1, key="yt_status")
    with col2:
        sort_by = st.selectbox("æ’åºæ–¹å¼", ["çˆ¬å–æ—¶é—´", "AIå æ¯”", "è®¢é˜…æ•°", "å¹³å‡è§‚çœ‹"], index=0, key="yt_sort")
    with col3:
        limit = st.number_input("æ˜¾ç¤ºæ•°é‡", min_value=10, max_value=1000, value=50, step=10, key="yt_limit")
    
    status_map = {"å…¨éƒ¨": None, "åˆæ ¼": "qualified", "å¾…åˆ†æ": "pending", "å·²æ‹’ç»": "rejected"}
    sort_map = {"çˆ¬å–æ—¶é—´": "discovered_at DESC", "AIå æ¯”": "ai_ratio DESC", "è®¢é˜…æ•°": "subscribers DESC", "å¹³å‡è§‚çœ‹": "avg_views DESC"}
    
    query = "SELECT * FROM youtube_kols"
    if status_filter != "å…¨éƒ¨":
        query += f" WHERE status = '{status_map[status_filter]}'"
    query += f" ORDER BY {sort_map[sort_by]} LIMIT {limit}"
    
    try:
        kols = st.session_state.db.fetchall(query)
    except Exception as e:
        st.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
        add_log(f"YouTubeæ•°æ®æŸ¥è¯¢å¤±è´¥: {str(e)}", "ERROR")
        
        # æä¾›ä¿®å¤é€‰é¡¹
        if st.button("å°è¯•ä¿®å¤æ•°æ®åº“", key="repair_db_yt"):
            if st.session_state.db.repair_database():
                st.success("æ•°æ®åº“ä¿®å¤æˆåŠŸï¼Œè¯·åˆ·æ–°é¡µé¢")
                add_log("æ•°æ®åº“ä¿®å¤æˆåŠŸ", "SUCCESS")
            else:
                st.error("æ•°æ®åº“ä¿®å¤å¤±è´¥")
                add_log("æ•°æ®åº“ä¿®å¤å¤±è´¥", "ERROR")
        return
    
    if kols:
        df = pd.DataFrame(kols)
        display_columns = ['channel_name', 'channel_url', 'subscribers', 'total_videos', 'ai_ratio',
                         'avg_views', 'avg_likes', 'avg_comments', 'engagement_rate', 'contact_info', 'status', 'discovered_at']
        display_df = df[display_columns].copy()
        display_df.columns = ['é¢‘é“åç§°', 'é¢‘é“é“¾æ¥', 'è®¢é˜…æ•°', 'æ€»è§†é¢‘', 'AIå æ¯”', 'å¹³å‡è§‚çœ‹', 'å¹³å‡ç‚¹èµ', 'å¹³å‡è¯„è®º', 'äº’åŠ¨ç‡', 'è”ç³»æ–¹å¼', 'çŠ¶æ€', 'çˆ¬å–æ—¶é—´']
        display_df = df[display_columns].copy()
        display_df.columns = ['é¢‘é“åç§°', 'é¢‘é“é“¾æ¥', 'è®¢é˜…æ•°', 'æ€»è§†é¢‘', 'AIå æ¯”', 'å¹³å‡è§‚çœ‹', 'å¹³å‡ç‚¹èµ', 'å¹³å‡è¯„è®º', 'äº’åŠ¨ç‡', 'è”ç³»æ–¹å¼', 'çŠ¶æ€', 'çˆ¬å–æ—¶é—´']
        
        display_df['æ€»è§†é¢‘'] = display_df['æ€»è§†é¢‘'].apply(lambda x: str(int(x)))
        display_df['AIå æ¯”'] = display_df['AIå æ¯”'].apply(lambda x: f"{x*100:.1f}%")
        display_df['äº’åŠ¨ç‡'] = display_df['äº’åŠ¨ç‡'].apply(lambda x: f"{x:.2f}%")
        display_df['è®¢é˜…æ•°'] = display_df['è®¢é˜…æ•°'].apply(lambda x: f"{x:,}")
        display_df['å¹³å‡è§‚çœ‹'] = display_df['å¹³å‡è§‚çœ‹'].apply(lambda x: f"{x:,}")
        display_df['å¹³å‡ç‚¹èµ'] = display_df['å¹³å‡ç‚¹èµ'].apply(lambda x: f"{x:,}")
        display_df['å¹³å‡è¯„è®º'] = display_df['å¹³å‡è¯„è®º'].apply(lambda x: f"{x:,}")
        display_df['è”ç³»æ–¹å¼'] = display_df['è”ç³»æ–¹å¼'].fillna('')
        
        # æ—¶é—´æ ¼å¼åŒ–ï¼ˆæ•°æ®åº“å·²å­˜å‚¨åŒ—äº¬æ—¶é—´ï¼Œç›´æ¥æ˜¾ç¤ºï¼‰
        def format_time(dt):
            if pd.isna(dt):
                return ""
            if isinstance(dt, str):
                try:
                    dt = pd.to_datetime(dt)
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    return dt
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        
        display_df['çˆ¬å–æ—¶é—´'] = display_df['çˆ¬å–æ—¶é—´'].apply(format_time)
        
        table_height = min(max(len(display_df) * 35 + 50, 200), 800)
        st.dataframe(display_df, width='stretch', hide_index=True, height=table_height,
                    column_config={"é¢‘é“é“¾æ¥": st.column_config.LinkColumn("é¢‘é“é“¾æ¥", help="ç‚¹å‡»æ‰“å¼€YouTubeé¢‘é“")})
        
        st.divider()
        
        # å¯¼å‡ºæŒ‰é’®åŒºåŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            # å¯¼å‡ºæ‰€æœ‰æ•°æ®
            if st.button("å¯¼å‡ºæ‰€æœ‰æ•°æ®", key="export_yt_all_data", use_container_width=True):
                try:
                    from tasks.youtube.export import YouTubeExportTask
                    export_task = YouTubeExportTask(st.session_state.youtube_repository)
                    filepath = export_task.run()
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            excel_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=os.path.basename(filepath),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            key="download_yt_all_excel_file"
                        )
                        add_log(f"å¯¼å‡ºExcelæˆåŠŸ: {filepath}", "SUCCESS")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºExcelå¤±è´¥: {str(e)}", "ERROR")
        
        with col2:
            # å¯¼å‡ºä»Šæ—¥æ•°æ®
            from datetime import datetime, timedelta, timezone
            beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
            today_str = beijing_time.strftime('%Y-%m-%d')
            
            if st.button(f"å¯¼å‡ºä»Šæ—¥æ•°æ® ({today_str})", key="export_yt_today_data", use_container_width=True):
                try:
                    from tasks.youtube.export import YouTubeExportTask
                    export_task = YouTubeExportTask(st.session_state.youtube_repository)
                    
                    # è®¡ç®—ä»Šå¤©çš„æ—¶é—´èŒƒå›´
                    today_start = beijing_time.replace(hour=0, minute=0, second=0, microsecond=0)
                    today_end = beijing_time.replace(hour=23, minute=59, second=59, microsecond=0)
                    
                    # å¯¼å‡ºä»Šæ—¥æ•°æ®
                    filepath = export_task.run_today(today_start, today_end)
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            excel_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½ä»Šæ—¥Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=os.path.basename(filepath),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            key="download_yt_today_excel_file"
                        )
                        add_log(f"å¯¼å‡ºä»Šæ—¥ExcelæˆåŠŸ: {filepath}", "SUCCESS")
                    else:
                        st.warning("ä»Šå¤©æš‚æ— æ•°æ®")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºä»Šæ—¥Excelå¤±è´¥: {str(e)}", "ERROR")
    else:
        st.info("æš‚æ— æ•°æ®")

def render_github_data_content():
    """æ¸²æŸ“GitHubæ•°æ®å†…å®¹"""
    if not st.session_state.github_repository:
        st.warning("è¯·å…ˆè¿æ¥æ•°æ®åº“")
        return
    
    # æ·»åŠ ç±»å‹é€‰æ‹©
    st.subheader("é€‰æ‹©æ•°æ®ç±»å‹")
    data_type = st.radio(
        "æ•°æ®ç±»å‹",
        ["ğŸ’¼ å•†ä¸š/ç‹¬ç«‹å¼€å‘è€…", "ğŸ“ å­¦æœ¯äººå£«"],
        horizontal=True,
        key="github_data_type"
    )
    
    st.divider()
    
    if data_type == "ğŸ’¼ å•†ä¸š/ç‹¬ç«‹å¼€å‘è€…":
        render_github_commercial_data()
    else:
        render_github_academic_data()

def render_github_commercial_data():
    """æ¸²æŸ“å•†ä¸šå¼€å‘è€…æ•°æ®"""
    col1, col2, col3 = st.columns(3)
    with col1:
        status_filter = st.selectbox("çŠ¶æ€ç­›é€‰", ["å…¨éƒ¨", "åˆæ ¼", "å¾…åˆ†æ", "å·²æ‹’ç»"], index=1, key="gh_commercial_status")
    with col2:
        sort_by = st.selectbox("æ’åºæ–¹å¼", ["çˆ¬å–æ—¶é—´", "æ€»Stars", "Followers", "ä»“åº“æ•°"], index=0, key="gh_commercial_sort")
    with col3:
        limit = st.number_input("æ˜¾ç¤ºæ•°é‡", min_value=10, max_value=1000, value=50, step=10, key="gh_commercial_limit")
    
    status_map = {"å…¨éƒ¨": None, "åˆæ ¼": "qualified", "å¾…åˆ†æ": "pending", "å·²æ‹’ç»": "rejected"}
    sort_map = {"çˆ¬å–æ—¶é—´": "discovered_at DESC", "æ€»Stars": "total_stars DESC", "Followers": "followers DESC", "ä»“åº“æ•°": "public_repos DESC"}
    
    query = "SELECT * FROM github_developers"
    if status_filter != "å…¨éƒ¨":
        query += f" WHERE status = '{status_map[status_filter]}'"
    query += f" ORDER BY {sort_map[sort_by]} LIMIT {limit}"
    
    try:
        devs = st.session_state.db.fetchall(query)
    except Exception as e:
        st.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
        add_log(f"GitHubå•†ä¸šæ•°æ®æŸ¥è¯¢å¤±è´¥: {str(e)}", "ERROR")
        
        if st.button("å°è¯•ä¿®å¤æ•°æ®åº“", key="repair_db_gh_commercial"):
            if st.session_state.db.repair_database():
                st.success("æ•°æ®åº“ä¿®å¤æˆåŠŸï¼Œè¯·åˆ·æ–°é¡µé¢")
                add_log("æ•°æ®åº“ä¿®å¤æˆåŠŸ", "SUCCESS")
            else:
                st.error("æ•°æ®åº“ä¿®å¤å¤±è´¥")
                add_log("æ•°æ®åº“ä¿®å¤å¤±è´¥", "ERROR")
        return
    
    if devs:
        df = pd.DataFrame(devs)
        display_columns = ['username', 'name', 'profile_url', 'followers', 'public_repos', 'total_stars', 'contact_info', 'status', 'discovered_at']
        display_df = df[display_columns].copy()
        display_df.columns = ['ç”¨æˆ·å', 'å§“å', 'ä¸»é¡µé“¾æ¥', 'Followers', 'ä»“åº“æ•°', 'æ€»Stars', 'è”ç³»æ–¹å¼', 'çŠ¶æ€', 'çˆ¬å–æ—¶é—´']
        
        display_df['Followers'] = display_df['Followers'].apply(lambda x: f"{x:,}")
        display_df['ä»“åº“æ•°'] = display_df['ä»“åº“æ•°'].apply(lambda x: f"{x:,}")
        display_df['æ€»Stars'] = display_df['æ€»Stars'].apply(lambda x: f"{x:,}")
        display_df['è”ç³»æ–¹å¼'] = display_df['è”ç³»æ–¹å¼'].fillna('')
        
        def format_time(dt):
            if pd.isna(dt):
                return ""
            if isinstance(dt, str):
                try:
                    dt = pd.to_datetime(dt)
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    return dt
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        
        display_df['çˆ¬å–æ—¶é—´'] = display_df['çˆ¬å–æ—¶é—´'].apply(format_time)
        
        table_height = min(max(len(display_df) * 35 + 50, 200), 800)
        st.dataframe(display_df, width='stretch', hide_index=True, height=table_height,
                    column_config={"ä¸»é¡µé“¾æ¥": st.column_config.LinkColumn("ä¸»é¡µé“¾æ¥", help="ç‚¹å‡»æ‰“å¼€GitHubä¸»é¡µ")})
        
        st.divider()
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("å¯¼å‡ºæ‰€æœ‰æ•°æ®", key="export_gh_commercial_all", use_container_width=True):
                try:
                    from tasks.github.export import GitHubExportTask
                    export_task = GitHubExportTask(st.session_state.github_repository)
                    filepath = export_task.run()
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            excel_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=os.path.basename(filepath),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            key="download_gh_commercial_excel"
                        )
                        add_log(f"å¯¼å‡ºå•†ä¸šå¼€å‘è€…ExcelæˆåŠŸ: {filepath}", "SUCCESS")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºå•†ä¸šå¼€å‘è€…Excelå¤±è´¥: {str(e)}", "ERROR")
        
        with col2:
            from datetime import datetime, timedelta, timezone
            beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
            today_str = beijing_time.strftime('%Y-%m-%d')
            
            if st.button(f"å¯¼å‡ºä»Šæ—¥æ•°æ® ({today_str})", key="export_gh_commercial_today", use_container_width=True):
                try:
                    from tasks.github.export import GitHubExportTask
                    export_task = GitHubExportTask(st.session_state.github_repository)
                    
                    today_start = beijing_time.replace(hour=0, minute=0, second=0, microsecond=0)
                    today_end = beijing_time.replace(hour=23, minute=59, second=59, microsecond=0)
                    
                    filepath = export_task.run_today(today_start, today_end)
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            excel_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½ä»Šæ—¥Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=os.path.basename(filepath),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            key="download_gh_commercial_today_excel"
                        )
                        add_log(f"å¯¼å‡ºä»Šæ—¥å•†ä¸šå¼€å‘è€…ExcelæˆåŠŸ: {filepath}", "SUCCESS")
                    else:
                        st.warning("ä»Šå¤©æš‚æ— æ•°æ®")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºä»Šæ—¥å•†ä¸šå¼€å‘è€…Excelå¤±è´¥: {str(e)}", "ERROR")
    else:
        st.info("æš‚æ— æ•°æ®")

def render_github_academic_data():
    """æ¸²æŸ“å­¦æœ¯äººå£«æ•°æ®"""
    if not st.session_state.github_academic_repository:
        st.warning("å­¦æœ¯äººå£«ä»“åº“æœªåˆå§‹åŒ–")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        status_filter = st.selectbox("çŠ¶æ€ç­›é€‰", ["å…¨éƒ¨", "åˆæ ¼", "å¾…åˆ†æ"], index=1, key="gh_academic_status")
    with col2:
        sort_by = st.selectbox("æ’åºæ–¹å¼", ["çˆ¬å–æ—¶é—´", "æ€»Stars", "Followers", "ä»“åº“æ•°"], index=0, key="gh_academic_sort")
    with col3:
        limit = st.number_input("æ˜¾ç¤ºæ•°é‡", min_value=10, max_value=1000, value=50, step=10, key="gh_academic_limit")
    
    status_map = {"å…¨éƒ¨": None, "åˆæ ¼": "qualified", "å¾…åˆ†æ": "pending"}
    sort_map = {"çˆ¬å–æ—¶é—´": "discovered_at DESC", "æ€»Stars": "total_stars DESC", "Followers": "followers DESC", "ä»“åº“æ•°": "public_repos DESC"}
    
    query = "SELECT * FROM github_academic_developers"
    if status_filter != "å…¨éƒ¨":
        query += f" WHERE status = '{status_map[status_filter]}'"
    query += f" ORDER BY {sort_map[sort_by]} LIMIT {limit}"
    
    try:
        devs = st.session_state.db.fetchall(query)
    except Exception as e:
        st.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
        add_log(f"GitHubå­¦æœ¯æ•°æ®æŸ¥è¯¢å¤±è´¥: {str(e)}", "ERROR")
        
        if st.button("å°è¯•ä¿®å¤æ•°æ®åº“", key="repair_db_gh_academic"):
            if st.session_state.db.repair_database():
                st.success("æ•°æ®åº“ä¿®å¤æˆåŠŸï¼Œè¯·åˆ·æ–°é¡µé¢")
                add_log("æ•°æ®åº“ä¿®å¤æˆåŠŸ", "SUCCESS")
            else:
                st.error("æ•°æ®åº“ä¿®å¤å¤±è´¥")
                add_log("æ•°æ®åº“ä¿®å¤å¤±è´¥", "ERROR")
        return
    
    if devs:
        df = pd.DataFrame(devs)
        display_columns = ['username', 'name', 'profile_url', 'followers', 'public_repos', 'total_stars', 'research_areas', 'contact_info', 'status', 'discovered_at']
        display_df = df[display_columns].copy()
        display_df.columns = ['ç”¨æˆ·å', 'å§“å', 'ä¸»é¡µé“¾æ¥', 'Followers', 'ä»“åº“æ•°', 'æ€»Stars', 'ç ”ç©¶é¢†åŸŸ', 'è”ç³»æ–¹å¼', 'çŠ¶æ€', 'çˆ¬å–æ—¶é—´']
        
        display_df['Followers'] = display_df['Followers'].apply(lambda x: f"{x:,}")
        display_df['ä»“åº“æ•°'] = display_df['ä»“åº“æ•°'].apply(lambda x: f"{x:,}")
        display_df['æ€»Stars'] = display_df['æ€»Stars'].apply(lambda x: f"{x:,}")
        
        def format_research_areas(areas_json):
            if pd.isna(areas_json) or not areas_json:
                return ""
            try:
                import json
                areas = json.loads(areas_json)
                return ", ".join(areas) if areas else ""
            except:
                return str(areas_json)
        
        display_df['ç ”ç©¶é¢†åŸŸ'] = display_df['ç ”ç©¶é¢†åŸŸ'].apply(format_research_areas)
        display_df['è”ç³»æ–¹å¼'] = display_df['è”ç³»æ–¹å¼'].fillna('')
        
        def format_time(dt):
            if pd.isna(dt):
                return ""
            if isinstance(dt, str):
                try:
                    dt = pd.to_datetime(dt)
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    return dt
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        
        display_df['çˆ¬å–æ—¶é—´'] = display_df['çˆ¬å–æ—¶é—´'].apply(format_time)
        
        table_height = min(max(len(display_df) * 35 + 50, 200), 800)
        st.dataframe(display_df, width='stretch', hide_index=True, height=table_height,
                    column_config={"ä¸»é¡µé“¾æ¥": st.column_config.LinkColumn("ä¸»é¡µé“¾æ¥", help="ç‚¹å‡»æ‰“å¼€GitHubä¸»é¡µ")})
        
        st.divider()
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("å¯¼å‡ºæ‰€æœ‰æ•°æ®", key="export_gh_academic_all", use_container_width=True):
                try:
                    from tasks.github.export_academic import GitHubAcademicExportTask
                    export_task = GitHubAcademicExportTask(st.session_state.github_academic_repository)
                    filepath = export_task.run()
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            excel_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=os.path.basename(filepath),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            key="download_gh_academic_excel"
                        )
                        add_log(f"å¯¼å‡ºå­¦æœ¯äººå£«ExcelæˆåŠŸ: {filepath}", "SUCCESS")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºå­¦æœ¯äººå£«Excelå¤±è´¥: {str(e)}", "ERROR")
        
        with col2:
            from datetime import datetime, timedelta, timezone
            beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
            today_str = beijing_time.strftime('%Y-%m-%d')
            
            if st.button(f"å¯¼å‡ºä»Šæ—¥æ•°æ® ({today_str})", key="export_gh_academic_today", use_container_width=True):
                try:
                    from tasks.github.export_academic import GitHubAcademicExportTask
                    export_task = GitHubAcademicExportTask(st.session_state.github_academic_repository)
                    
                    today_start = beijing_time.replace(hour=0, minute=0, second=0, microsecond=0)
                    today_end = beijing_time.replace(hour=23, minute=59, second=59, microsecond=0)
                    
                    filepath = export_task.run_today(today_start, today_end)
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            excel_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½ä»Šæ—¥Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=os.path.basename(filepath),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            key="download_gh_academic_today_excel"
                        )
                        add_log(f"å¯¼å‡ºä»Šæ—¥å­¦æœ¯äººå£«ExcelæˆåŠŸ: {filepath}", "SUCCESS")
                    else:
                        st.warning("ä»Šå¤©æš‚æ— æ•°æ®")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºä»Šæ—¥å­¦æœ¯äººå£«Excelå¤±è´¥: {str(e)}", "ERROR")
    else:
        st.info("æš‚æ— æ•°æ®")

def render_twitter_data_content():
    """æ¸²æŸ“Twitteræ•°æ®å†…å®¹"""
    if not st.session_state.twitter_repository:
        st.warning("è¯·å…ˆè¿æ¥æ•°æ®åº“")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        status_filter = st.selectbox("çŠ¶æ€ç­›é€‰", ["å…¨éƒ¨", "åˆæ ¼", "å¾…åˆ†æ"], index=1, key="tw_status")
    with col2:
        sort_by = st.selectbox("æ’åºæ–¹å¼", ["çˆ¬å–æ—¶é—´", "è´¨é‡åˆ†æ•°", "ç²‰ä¸æ•°", "æ¨æ–‡æ•°"], index=0, key="tw_sort")
    with col3:
        limit = st.number_input("æ˜¾ç¤ºæ•°é‡", min_value=10, max_value=1000, value=50, step=10, key="tw_limit")
    
    status_map = {"å…¨éƒ¨": None, "åˆæ ¼": "qualified", "å¾…åˆ†æ": "pending"}
    sort_map = {"çˆ¬å–æ—¶é—´": "discovered_at DESC", "è´¨é‡åˆ†æ•°": "quality_score DESC", "ç²‰ä¸æ•°": "followers_count DESC", "æ¨æ–‡æ•°": "tweet_count DESC"}
    
    query = "SELECT * FROM twitter_users"
    if status_filter != "å…¨éƒ¨":
        query += f" WHERE status = '{status_map[status_filter]}'"
    query += f" ORDER BY {sort_map[sort_by]} LIMIT {limit}"
    
    try:
        users = st.session_state.db.fetchall(query)
    except Exception as e:
        st.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
        add_log(f"Twitteræ•°æ®æŸ¥è¯¢å¤±è´¥: {str(e)}", "ERROR")
        
        # æä¾›ä¿®å¤é€‰é¡¹
        if st.button("å°è¯•ä¿®å¤æ•°æ®åº“", key="repair_db_tw"):
            if st.session_state.db.repair_database():
                st.success("æ•°æ®åº“ä¿®å¤æˆåŠŸï¼Œè¯·åˆ·æ–°é¡µé¢")
                add_log("æ•°æ®åº“ä¿®å¤æˆåŠŸ", "SUCCESS")
            else:
                st.error("æ•°æ®åº“ä¿®å¤å¤±è´¥")
                add_log("æ•°æ®åº“ä¿®å¤å¤±è´¥", "ERROR")
        return
    
    if users:
        df = pd.DataFrame(users)
        display_columns = ['username', 'name', 'followers_count', 'tweet_count', 'ai_ratio', 
                         'quality_score', 'avg_engagement', 'verified', 'contact_info', 'status', 'discovered_at']
        display_df = df[display_columns].copy()
        display_df.columns = ['ç”¨æˆ·å', 'å§“å', 'ç²‰ä¸æ•°', 'æ¨æ–‡æ•°', 'AIç›¸å…³åº¦', 'è´¨é‡åˆ†æ•°', 'å¹³å‡äº’åŠ¨', 'è®¤è¯', 'è”ç³»æ–¹å¼', 'çŠ¶æ€', 'çˆ¬å–æ—¶é—´']
        
        display_df['ç²‰ä¸æ•°'] = display_df['ç²‰ä¸æ•°'].apply(lambda x: f"{x:,}")
        display_df['æ¨æ–‡æ•°'] = display_df['æ¨æ–‡æ•°'].apply(lambda x: f"{x:,}")
        display_df['AIç›¸å…³åº¦'] = display_df['AIç›¸å…³åº¦'].apply(lambda x: f"{x*100:.1f}%")
        display_df['è´¨é‡åˆ†æ•°'] = display_df['è´¨é‡åˆ†æ•°'].apply(lambda x: f"{x:.1f}")
        display_df['å¹³å‡äº’åŠ¨'] = display_df['å¹³å‡äº’åŠ¨'].apply(lambda x: f"{x:.1f}")
        display_df['è®¤è¯'] = display_df['è®¤è¯'].apply(lambda x: 'å·²è®¤è¯' if x == 1 else 'æœªè®¤è¯')
        display_df['è”ç³»æ–¹å¼'] = display_df['è”ç³»æ–¹å¼'].fillna('')
        
        # æ—¶é—´æ ¼å¼åŒ–
        def format_time(dt):
            if pd.isna(dt):
                return ""
            if isinstance(dt, str):
                try:
                    dt = pd.to_datetime(dt)
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    return dt
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        
        display_df['çˆ¬å–æ—¶é—´'] = display_df['çˆ¬å–æ—¶é—´'].apply(format_time)
        
        table_height = min(max(len(display_df) * 35 + 50, 200), 800)
        st.dataframe(display_df, width='stretch', hide_index=True, height=table_height)
        
        st.divider()
        
        # å¯¼å‡ºæŒ‰é’®åŒºåŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            # å¯¼å‡ºæ‰€æœ‰æ•°æ®
            if st.button("å¯¼å‡ºæ‰€æœ‰æ•°æ®", key="export_tw_all_data", use_container_width=True):
                try:
                    from tasks.twitter.export import TwitterExportTask
                    export_task = TwitterExportTask()
                    filepath = export_task.export_qualified_users(limit=1000)
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            excel_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=os.path.basename(filepath),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            key="download_tw_all_excel_file"
                        )
                        add_log(f"å¯¼å‡ºExcelæˆåŠŸ: {filepath}", "SUCCESS")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºExcelå¤±è´¥: {str(e)}", "ERROR")
        
        with col2:
            # å¯¼å‡ºCSV
            if st.button("å¯¼å‡ºCSV", key="export_tw_csv", use_container_width=True):
                try:
                    from tasks.twitter.export import TwitterExportTask
                    export_task = TwitterExportTask()
                    filepath = export_task.export_all_users_csv()
                    if filepath and os.path.exists(filepath):
                        with open(filepath, 'rb') as f:
                            csv_data = f.read()
                        st.download_button(
                            label="ä¸‹è½½CSVæ–‡ä»¶",
                            data=csv_data,
                            file_name=os.path.basename(filepath),
                            mime="text/csv",
                            use_container_width=True,
                            key="download_tw_csv_file"
                        )
                        add_log(f"å¯¼å‡ºCSVæˆåŠŸ: {filepath}", "SUCCESS")
                except Exception as e:
                    st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
                    add_log(f"å¯¼å‡ºCSVå¤±è´¥: {str(e)}", "ERROR")
    else:
        st.info("æš‚æ— æ•°æ®")

def render_logs():
    """æ¸²æŸ“æ—¥å¿—æŸ¥çœ‹é¡µé¢"""
    st.markdown('<div class="main-header">å®æ—¶æ—¥å¿—</div>', unsafe_allow_html=True)
    
    # æ£€æŸ¥å¹¶ä¿®å¤çŠ¶æ€
    check_and_fix_crawler_status()
    
    if 'auto_refresh_enabled' not in st.session_state:
        st.session_state.auto_refresh_enabled = True
    
    crawler_is_running = is_crawler_running()
    
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        if st.button("åˆ·æ–°æ—¥å¿—", key="refresh_logs_btn"):
            st.rerun()
    with col2:
        if st.button("æ¸…ç©ºæ—¥å¿—", key="clear_logs_btn"):
            if clear_logs():
                st.success("æ—¥å¿—å·²æ¸…ç©º")
                time.sleep(0.5)
                st.rerun()
            else:
                st.error("æ¸…ç©ºæ—¥å¿—å¤±è´¥")
    with col3:
        auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–° (æ¯3ç§’)", value=st.session_state.auto_refresh_enabled,
                                   key="auto_refresh_checkbox_unique", help="çˆ¬è™«è¿è¡Œæ—¶è‡ªåŠ¨åˆ·æ–°æ—¥å¿—")
        if auto_refresh != st.session_state.auto_refresh_enabled:
            st.session_state.auto_refresh_enabled = auto_refresh
    
    st.divider()
    
    # ä½¿ç”¨åŒ—äº¬æ—¶é—´è·å–æ—¥å¿—æ–‡ä»¶
    from datetime import datetime, timedelta, timezone
    beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
    log_file = os.path.join(LOG_DIR, f"{beijing_time.strftime('%Y%m%d')}.log")
    all_logs = []
    
    if os.path.exists(log_file):
        try:
            encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']
            for encoding in encodings:
                try:
                    with open(log_file, 'r', encoding=encoding) as f:
                        file_logs = f.readlines()
                        all_logs = [line.strip() for line in file_logs if line.strip()]
                    break
                except UnicodeDecodeError:
                    continue
        except Exception as e:
            st.error(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
    
    log_count = len(all_logs)
    display_count = min(log_count, 200)
    
    if crawler_is_running:
        st.markdown(f"""
        <div style="background: linear-gradient(90deg, #1e3a8a 0%, #3b82f6 100%); 
                    padding: 15px; border-radius: 10px; margin-bottom: 10px;">
            <div style="display: flex; align-items: center; justify-content: space-between;">
                <div style="display: flex; align-items: center;">
                    <div style="width: 12px; height: 12px; background: #22c55e; 
                                border-radius: 50%; margin-right: 10px;"></div>
                    <span style="color: white; font-size: 18px; font-weight: bold;">
                        çˆ¬è™«è¿è¡Œä¸­...
                    </span>
                </div>
                <span style="color: #e0e7ff; font-size: 14px;">
                    å…± {log_count} æ¡æ—¥å¿— | æ˜¾ç¤ºæœ€è¿‘ {display_count} æ¡
                </span>
            </div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div style="background: linear-gradient(90deg, #065f46 0%, #10b981 100%); 
                    padding: 15px; border-radius: 10px; margin-bottom: 10px;">
            <div style="display: flex; align-items: center; justify-content: space-between;">
                <div style="display: flex; align-items: center;">
                    <span style="color: white; font-size: 18px; font-weight: bold;">
                        çˆ¬è™«å·²åœæ­¢
                    </span>
                </div>
                <span style="color: #d1fae5; font-size: 14px;">
                    å…± {log_count} æ¡æ—¥å¿— | æ˜¾ç¤ºæœ€è¿‘ {display_count} æ¡
                </span>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    if all_logs:
        logs_text = "\n".join(all_logs[-200:])
        
        log_container_id = f"log_container_{int(time.time() * 1000)}"
        import html
        logs_html = html.escape(logs_text)
        
        st.markdown(f'<div class="log-container" id="{log_container_id}">{logs_html}</div>', unsafe_allow_html=True)
        
        st.markdown(f"""
        <script>
        setTimeout(function() {{
            var container = document.getElementById('{log_container_id}');
            if (container) {{
                container.scrollTop = container.scrollHeight;
            }}
        }}, 100);
        </script>
        """, unsafe_allow_html=True)
    else:
        st.info("æš‚æ— æ—¥å¿—è®°å½•")
    
    if st.session_state.auto_refresh_enabled and crawler_is_running:
        time.sleep(3)
        st.rerun()

def render_ai_rules():
    """æ¸²æŸ“YouTube AIè§„åˆ™é…ç½®é¡µé¢"""
    from ui.youtube import render_rules
    render_rules(PROJECT_ROOT, add_log)

def render_github_rules():
    """æ¸²æŸ“GitHubè§„åˆ™é…ç½®é¡µé¢"""
    from ui.github import render_rules
    render_rules(PROJECT_ROOT, add_log)

def render_settings():
    """æ¸²æŸ“è®¾ç½®é¡µé¢"""
    st.markdown('<div class="main-header">ç³»ç»Ÿè®¾ç½®</div>', unsafe_allow_html=True)
    
    st.subheader("æ•°æ®åº“ä¿¡æ¯")
    st.info("å½“å‰ä½¿ç”¨SQLiteæ•°æ®åº“ï¼Œæ•°æ®ä¿å­˜åœ¨ data/ai_kol_crawler.db")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("æŸ¥çœ‹æ•°æ®åº“å¤§å°", use_container_width=True):
            db_path = 'data/ai_kol_crawler.db'
            if os.path.exists(db_path):
                size = os.path.getsize(db_path) / 1024 / 1024
                st.info(f"æ•°æ®åº“å¤§å°: {size:.2f} MB")
            else:
                st.warning("æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
    
    with col2:
        if st.button("å¤‡ä»½æ•°æ®åº“", use_container_width=True):
            import shutil
            from datetime import datetime, timedelta, timezone
            try:
                backup_dir = "backups"
                os.makedirs(backup_dir, exist_ok=True)
                beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
                backup_name = f"{backup_dir}/backup_{beijing_time.strftime('%Y%m%d_%H%M%S')}.db"
                shutil.copy('data/ai_kol_crawler.db', backup_name)
                st.success(f"å¤‡ä»½æˆåŠŸ: {backup_name}")
            except Exception as e:
                st.error(f"å¤‡ä»½å¤±è´¥: {e}")
    
    with col3:
        if st.button("ä¿®å¤æ•°æ®åº“", use_container_width=True):
            if st.session_state.db:
                with st.spinner("æ­£åœ¨ä¿®å¤æ•°æ®åº“..."):
                    if st.session_state.db.repair_database():
                        st.success("æ•°æ®åº“ä¿®å¤æˆåŠŸ")
                        add_log("æ•°æ®åº“ä¿®å¤æˆåŠŸ", "SUCCESS")
                    else:
                        st.error("æ•°æ®åº“ä¿®å¤å¤±è´¥")
                        add_log("æ•°æ®åº“ä¿®å¤å¤±è´¥", "ERROR")
            else:
                st.warning("æ•°æ®åº“æœªè¿æ¥")
    
    st.divider()
    
    st.subheader("ç³»ç»Ÿä¿¡æ¯")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ç‰ˆæœ¬ä¿¡æ¯**")
        st.write("- ç³»ç»Ÿç‰ˆæœ¬: v2.0")
        st.write("- æ•°æ®åº“: SQLite")
        st.write("- Python:", sys.version.split()[0])
    
    with col2:
        st.write("**ç»Ÿè®¡ä¿¡æ¯**")
        youtube_stats = get_statistics('youtube')
        github_stats = get_statistics('github')
        twitter_stats = get_statistics('twitter')
        st.write(f"- YouTube KOL: {youtube_stats.get('qualified_kols', 0)}")
        st.write(f"- GitHubå¼€å‘è€…: {github_stats.get('qualified_developers', 0)}")
        st.write(f"- Twitterç”¨æˆ·: {twitter_stats.get('qualified_users', 0)}")


if __name__ == "__main__":
    """ä¸»ç¨‹åº"""
    init_session_state()
    
    with st.sidebar:
        st.markdown("### å¤šå¹³å°çˆ¬è™«")
        
        if connect_database():
            st.success("å·²è¿æ¥")
        else:
            st.error("æœªè¿æ¥")
        
        # å¿«é€Ÿç»Ÿè®¡ - æ›´ç´§å‡‘
        youtube_stats = get_statistics('youtube')
        github_stats = get_statistics('github')
        # twitter_stats = get_statistics('twitter')  # æš‚æ—¶ç¦ç”¨
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**YouTube**")
            st.markdown(f"<div class='stat-number'>{youtube_stats.get('qualified_kols', 0)}</div>", unsafe_allow_html=True)
        with col2:
            st.markdown("**GitHub**")
            st.markdown(f"<div class='stat-number'>{github_stats.get('qualified_developers', 0)}</div>", unsafe_allow_html=True)
        # with col3:
        #     st.markdown("**Twitter**")
        #     st.markdown(f"<div class='stat-number'>{twitter_stats.get('qualified_users', 0)}</div>", unsafe_allow_html=True)
        
        st.divider()
        
        # æ•°æ®æµè§ˆï¼ˆæœ€å‰é¢ï¼‰
        is_active = st.session_state.current_page == "data_browser"
        if st.button(
            "æ•°æ®æµè§ˆ", 
            key="btn_data_browser", 
            use_container_width=True,
            type="primary" if is_active else "secondary"
        ):
            st.session_state.current_page = "data_browser"
            st.rerun()
        
        # YouTubeåˆ†ç±»
        st.markdown("### YouTube")
        youtube_pages = {
            "youtube_dashboard": "ä»ªè¡¨ç›˜",
            "youtube_crawler": "çˆ¬è™«",
            "youtube_ai_rules": "è§„åˆ™"
        }
        
        for page_key, page_name in youtube_pages.items():
            is_active = st.session_state.current_page == page_key
            if st.button(
                page_name, 
                key=f"btn_{page_key}", 
                use_container_width=True,
                type="primary" if is_active else "secondary"
            ):
                st.session_state.current_page = page_key
                st.rerun()
        
        # GitHubåˆ†ç±»
        st.markdown("### GitHub")
        github_pages = {
            "github_dashboard": "ä»ªè¡¨ç›˜",
            "github_crawler": "çˆ¬è™«",
            "github_rules": "è§„åˆ™"
        }
        
        for page_key, page_name in github_pages.items():
            is_active = st.session_state.current_page == page_key
            if st.button(
                page_name, 
                key=f"btn_{page_key}", 
                use_container_width=True,
                type="primary" if is_active else "secondary"
            ):
                st.session_state.current_page = page_key
                st.rerun()
        
        # Twitteråˆ†ç±» - æš‚æ—¶ç¦ç”¨(åŠŸèƒ½æœªå®Œå–„)
        # st.markdown("### Twitter/X")
        # twitter_pages = {
        #     "twitter_dashboard": "ä»ªè¡¨ç›˜",
        #     "twitter_crawler": "çˆ¬è™«"
        # }
        # 
        # for page_key, page_name in twitter_pages.items():
        #     is_active = st.session_state.current_page == page_key
        #     if st.button(
        #         page_name, 
        #         key=f"btn_{page_key}", 
        #         use_container_width=True,
        #         type="primary" if is_active else "secondary"
        #     ):
        #         st.session_state.current_page = page_key
        #         st.rerun()
        
        st.divider()
        
        # æ—¥å¿—æŸ¥çœ‹å’Œè®¾ç½®ï¼ˆæœ€åé¢ï¼‰
        system_pages = {
            "logs": "æ—¥å¿—æŸ¥çœ‹",
            "settings": "è®¾ç½®"
        }
        
        for page_key, page_name in system_pages.items():
            is_active = st.session_state.current_page == page_key
            if st.button(
                page_name, 
                key=f"btn_{page_key}", 
                use_container_width=True,
                type="primary" if is_active else "secondary"
            ):
                st.session_state.current_page = page_key
                st.rerun()
        
        st.divider()
        
        if is_crawler_running():
            st.warning("çˆ¬è™«è¿è¡Œä¸­...")
        
        st.caption("å¤šå¹³å°çˆ¬è™«ç³»ç»Ÿ v2.0")
        st.caption("Â© 2026 All Rights Reserved")
    
    # å¤„ç†è·³è½¬åˆ°æ—¥å¿—
    if st.session_state.jump_to_logs:
        st.session_state.current_page = "logs"
        st.session_state.jump_to_logs = False
    
    # ä¸»å†…å®¹åŒº
    page = st.session_state.current_page
    
    if page == "youtube_dashboard":
        render_youtube_dashboard()
    elif page == "youtube_crawler":
        render_youtube_crawler()
    elif page == "youtube_ai_rules":
        render_ai_rules()
    elif page == "github_dashboard":
        render_github_dashboard()
    elif page == "github_crawler":
        render_github_crawler()
    elif page == "github_rules":
        render_github_rules()
    # Twitter é¡µé¢æš‚æ—¶ç¦ç”¨(åŠŸèƒ½æœªå®Œå–„)
    # elif page == "twitter_dashboard":
    #     render_twitter_dashboard()
    # elif page == "twitter_crawler":
    #     render_twitter_crawler()
    elif page == "data_browser":
        render_data_browser()
    elif page == "logs":
        render_logs()
    elif page == "settings":
        render_settings()
    else:
        render_youtube_dashboard()
