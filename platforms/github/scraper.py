# -*- coding: utf-8 -*-
"""
GitHubçˆ¬è™« - ä½¿ç”¨APIè·å–è´¡çŒ®è€…

ç­–ç•¥æ›´æ–°ï¼ˆ2026-01-30ï¼‰ï¼š
- ä½¿ç”¨GitHubå†…éƒ¨API (/graphs/contributors-data) è·å–è´¡çŒ®è€…
- é€Ÿåº¦å¿«ã€ç¨³å®šã€å¯è·å–å®Œæ•´åˆ—è¡¨ï¼ˆ100+ä¸ªè´¡çŒ®è€…ï¼‰
- ä¸å†ä½¿ç”¨Seleniumæˆ–ä¾§è¾¹æ çˆ¬å–
"""
import requests
import time
import re
import random
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
from utils.logger import setup_logger
from utils.retry import retry_on_failure

logger = setup_logger()


class GitHubScraper:
    """GitHubçˆ¬è™«ï¼ˆUAè½®æ¢+æ™ºèƒ½å»¶è¿Ÿï¼‰"""
    
    def __init__(self):
        self.session = requests.Session()
        
        # 20ä¸ªUser-Agent
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        ]
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–é€Ÿç‡é™åˆ¶å‚æ•°
        from utils.config_loader import load_config
        config = load_config()
        rate_limit_config = config.get('github', {}).get('rate_limit', {})
        
        self.min_delay = rate_limit_config.get('min_delay', 4.0)
        self.max_delay = rate_limit_config.get('max_delay', 7.0)
        self.initial_cooldown = rate_limit_config.get('initial_cooldown', 5)
        self.max_429_backoff = rate_limit_config.get('max_429_backoff', 30)
        
        self.last_request_time = 0
        self.rate_limit_count = 0
        self.consecutive_429 = 0  # è¿ç»­429æ¬¡æ•°
        
        logger.info(f"çˆ¬è™«åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿ{self.min_delay}-{self.max_delay}ç§’ï¼Œ{len(self.user_agents)}ä¸ªUAï¼‰")
        logger.info(f"â³ ç­‰å¾…{self.initial_cooldown}ç§’è®©IPå†·å´...")
        time.sleep(self.initial_cooldown)
        logger.info("âœ“ å†·å´å®Œæˆï¼Œå¼€å§‹çˆ¬å–")
    
    def _get_headers(self):
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml',
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive'
        }
    
    def _wait(self):
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        delay = random.uniform(self.min_delay, self.max_delay)
        
        # å¦‚æœè¿ç»­è§¦å‘429ï¼Œå¢åŠ é¢å¤–å»¶è¿Ÿ
        if self.consecutive_429 > 0:
            penalty = min(self.consecutive_429 * 2, 5)
            delay = min(delay + penalty, 5.0)
            logger.debug(f"è¿ç»­429 {self.consecutive_429}æ¬¡ï¼Œå»¶è¿Ÿå¢åŠ åˆ°{delay:.1f}ç§’")
        
        if time_since_last < delay:
            time.sleep(delay - time_since_last)
        
        self.last_request_time = time.time()
    
    @retry_on_failure(max_retries=3)
    def search_repositories(self, keyword: str, max_results: int = 10, sort: str = 'stars') -> List[Dict]:
        self._wait()
        
        try:
            url = "https://github.com/search"
            params = {'q': keyword, 'type': 'repositories', 's': sort, 'o': 'desc'}
            
            response = self.session.get(url, params=params, headers=self._get_headers(), timeout=15)
            
            if response.status_code == 429:
                self.consecutive_429 += 1
                # æŒ‡æ•°é€€é¿ï¼š2^nç§’ï¼Œæœ€å¤šä½¿ç”¨é…ç½®çš„æœ€å¤§å€¼
                wait_time = min(2 ** self.consecutive_429, self.max_429_backoff)
                logger.warning(f"âš ï¸ 429é”™è¯¯ï¼ˆç¬¬{self.consecutive_429}æ¬¡ï¼‰ï¼Œç­‰å¾…{wait_time}ç§’...")
                time.sleep(wait_time)
                
                # å¦‚æœè¿ç»­3æ¬¡429ï¼Œå»ºè®®æ›´é•¿çš„å†·å´æœŸ
                if self.consecutive_429 >= 3:
                    logger.warning(f"âš ï¸ è¿ç»­{self.consecutive_429}æ¬¡429ï¼Œå»ºè®®ç¨åå†è¯•æˆ–é™ä½çˆ¬å–é¢‘ç‡")
                    logger.warning(f"   å½“å‰å»¶è¿Ÿ: {self.min_delay}-{self.max_delay}ç§’ï¼Œæœ€å¤§é€€é¿: {self.max_429_backoff}ç§’")
                
                return self.search_repositories(keyword, max_results, sort)
            
            response.raise_for_status()
            
            # æˆåŠŸåé‡ç½®429è®¡æ•°
            if self.consecutive_429 > 0:
                logger.info(f"âœ“ é€Ÿç‡é™åˆ¶å·²è§£é™¤ï¼ˆä¹‹å‰è¿ç»­{self.consecutive_429}æ¬¡429ï¼‰")
                self.consecutive_429 = 0
            
            soup = BeautifulSoup(response.text, 'html.parser')
            repositories = []
            
            repo_items = soup.select('div[data-testid="results-list"] > div')
            if not repo_items:
                repo_items = soup.select('.repo-list-item')
            if not repo_items:
                repo_items = soup.select('li.repo-list-item')
            
            for item in repo_items[:max_results * 2]:
                try:
                    repo_link = item.select_one('a[href^="/"]')
                    if not repo_link:
                        continue
                    
                    href = repo_link.get('href', '')
                    if not href or '/topics' in href or '/search' in href:
                        continue
                    
                    repo_name = href.strip('/').split('?')[0]
                    parts = repo_name.split('/')
                    if len(parts) >= 2:
                        repo_name = f"{parts[0]}/{parts[1]}"
                    else:
                        continue
                    
                    owner_username = parts[0]
                    
                    desc_elem = item.select_one('p')
                    description = desc_elem.text.strip() if desc_elem else ''
                    
                    stars = 0
                    stars_elem = item.select_one('a[href*="stargazers"]')
                    if stars_elem:
                        stars_text = stars_elem.text.strip().replace(',', '')
                        if 'k' in stars_text.lower():
                            stars = int(float(stars_text.lower().replace('k', '')) * 1000)
                        else:
                            try:
                                stars = int(stars_text)
                            except:
                                pass
                    
                    lang_elem = item.select_one('[itemprop="programmingLanguage"]')
                    language = lang_elem.text.strip() if lang_elem else ''
                    
                    repositories.append({
                        'repo_id': hash(repo_name),
                        'repo_name': repo_name,
                        'repo_url': f"https://github.com/{repo_name}",
                        'owner_username': owner_username,
                        'owner_url': f"https://github.com/{owner_username}",
                        'description': description,
                        'stars': stars,
                        'forks': 0,
                        'language': language,
                        'created_at': None,
                        'updated_at': None
                    })
                    
                    if len(repositories) >= max_results:
                        break
                except:
                    continue
            
            logger.info(f"æœç´¢'{keyword}'æ‰¾åˆ°{len(repositories)}ä¸ªä»“åº“")
            return repositories
            
        except Exception as e:
            if '429' in str(e):
                self.consecutive_429 += 1
                wait_time = min(2 ** self.consecutive_429, 5)
                logger.warning(f"429é”™è¯¯ï¼ˆç¬¬{self.consecutive_429}æ¬¡ï¼‰ï¼Œç­‰å¾…{wait_time}ç§’...")
                time.sleep(wait_time)
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    @retry_on_failure(max_retries=3)
    def get_user_info(self, username: str) -> Optional[Dict]:
        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        from utils.crawler_status import should_stop
        if should_stop():
            logger.debug(f"æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡è·å–ç”¨æˆ·ä¿¡æ¯: {username}")
            return None
        
        self._wait()
        
        try:
            url = f"https://github.com/{username}"
            response = self.session.get(url, headers=self._get_headers(), timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            user_info = {
                'user_id': hash(username),
                'username': username,
                'name': '',
                'profile_url': url,
                'avatar_url': '',
                'bio': '',
                'company': '',
                'location': '',
                'email': '',
                'blog': '',
                'twitter': '',
                'public_repos': 0,
                'followers': 0,
                'following': 0,
                'created_at': None,
                'updated_at': None
            }
            
            name_elem = soup.select_one('span[itemprop="name"]')
            if name_elem:
                user_info['name'] = name_elem.text.strip()
            
            avatar_elem = soup.select_one('img[alt*="@"]')
            if avatar_elem:
                user_info['avatar_url'] = avatar_elem.get('src', '')
            
            bio_elem = soup.select_one('[data-bio-text]')
            if bio_elem:
                user_info['bio'] = bio_elem.text.strip()
            
            company_elem = soup.select_one('[itemprop="worksFor"]')
            if company_elem:
                user_info['company'] = company_elem.text.strip()
            
            location_elem = soup.select_one('[itemprop="homeLocation"]')
            if location_elem:
                user_info['location'] = location_elem.text.strip()
            
            blog_elem = soup.select_one('[itemprop="url"]')
            if blog_elem:
                user_info['blog'] = blog_elem.get('href', '')
            
            # æå–é‚®ç®± - å¤šç§æ–¹å¼
            # æ–¹å¼1ï¼šä»itemprop="email"æ ‡ç­¾æå–ï¼ˆæœ€å‡†ç¡®ï¼‰
            email_elem = soup.select_one('li[itemprop="email"] a[href^="mailto:"]')
            if email_elem:
                user_info['email'] = email_elem.text.strip()
            
            # æ–¹å¼2ï¼šä»ä»»ä½•mailtoé“¾æ¥æå–
            if not user_info['email']:
                email_elem = soup.select_one('a[href^="mailto:"]')
                if email_elem:
                    user_info['email'] = email_elem.get('href', '').replace('mailto:', '')
            
            # æ–¹å¼3ï¼šä»bioæˆ–å…¶ä»–æ–‡æœ¬ä¸­æå–
            if not user_info['email']:
                bio_text = user_info.get('bio', '')
                email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
                email_match = re.search(email_pattern, bio_text)
                if email_match:
                    user_info['email'] = email_match.group(0)
            
            # æ–¹å¼4ï¼šæš‚ä¸ä»commitæå–ï¼Œç•™åˆ°æœ€ååˆ¤æ–­åˆæ ¼åå†æå–
            # è¿™æ ·å¯ä»¥é¿å…å¯¹ä¸åˆæ ¼çš„å¼€å‘è€…æµªè´¹APIè°ƒç”¨
            
            # æå–åšå®¢/ç½‘ç«™
            blog_elem = soup.select_one('li[itemprop="url"] a[rel*="nofollow"]')
            if blog_elem:
                user_info['blog'] = blog_elem.get('href', '')
            elif not user_info['blog']:
                # å¤‡ç”¨æ–¹æ¡ˆ
                blog_elem = soup.select_one('[itemprop="url"]')
                if blog_elem:
                    user_info['blog'] = blog_elem.get('href', '')
            
            # æå–Twitter/X
            twitter_elem = soup.select_one('a[href*="twitter.com"], a[href*="x.com"]')
            if twitter_elem:
                twitter_url = twitter_elem.get('href', '')
                twitter_match = re.search(r'(?:twitter\.com|x\.com)/([^/?]+)', twitter_url)
                if twitter_match:
                    user_info['twitter'] = twitter_match.group(1)
            
            followers_elem = soup.select_one('a[href*="followers"] span')
            if followers_elem:
                try:
                    followers_text = followers_elem.text.strip().replace(',', '').lower()
                    if 'k' in followers_text:
                        user_info['followers'] = int(float(followers_text.replace('k', '')) * 1000)
                    elif 'm' in followers_text:
                        user_info['followers'] = int(float(followers_text.replace('m', '')) * 1000000)
                    else:
                        user_info['followers'] = int(followers_text)
                except:
                    pass
            
            following_elem = soup.select_one('a[href*="following"] span')
            if following_elem:
                try:
                    following_text = following_elem.text.strip().replace(',', '').lower()
                    if 'k' in following_text:
                        user_info['following'] = int(float(following_text.replace('k', '')) * 1000)
                    elif 'm' in following_text:
                        user_info['following'] = int(float(following_text.replace('m', '')) * 1000000)
                    else:
                        user_info['following'] = int(following_text)
                except:
                    pass
            
            repos_elem = soup.select_one('a[data-tab-item="repositories"] span')
            if repos_elem:
                try:
                    user_info['public_repos'] = int(repos_elem.text.strip().replace(',', ''))
                except:
                    pass
            
            logger.info(f"è·å–ç”¨æˆ·{username}æˆåŠŸ")
            return user_info
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·å¤±è´¥{username}: {e}")
            return None
    
    def _extract_email_from_commits(self, username: str) -> Optional[str]:
        """
        ä»ç”¨æˆ·çš„commitè®°å½•ä¸­æå–é‚®ç®±ï¼ˆé«˜æ•ˆç‰ˆæœ¬ï¼‰
        
        ç­–ç•¥ï¼š
        - åªæ£€æŸ¥æœ€è¿‘æ›´æ–°çš„2ä¸ªä»“åº“
        - æ¯ä¸ªä»“åº“åªè·å–æœ€è¿‘5ä¸ªcommits
        - æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆé‚®ç®±å°±è¿”å›
        
        Args:
            username: GitHubç”¨æˆ·å
            
        Returns:
            é‚®ç®±åœ°å€æˆ–None
        """
        try:
            # 1. è·å–ç”¨æˆ·æœ€è¿‘çš„2ä¸ªä»“åº“ï¼ˆAPIæ–¹å¼ï¼Œæ›´å¿«ï¼‰
            repos_url = f"https://api.github.com/users/{username}/repos"
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'application/vnd.github.v3+json'
            }
            params = {'sort': 'updated', 'per_page': 2}
            
            response = self.session.get(repos_url, headers=headers, params=params, timeout=10)
            
            if response.status_code != 200:
                return None
            
            repos = response.json()
            
            if not isinstance(repos, list) or not repos:
                return None
            
            # 2. å¯¹æ¯ä¸ªä»“åº“ï¼Œè·å–æœ€è¿‘çš„commits
            for repo in repos:
                repo_full_name = repo.get('full_name')
                if not repo_full_name:
                    continue
                
                # è·å–è¯¥ä»“åº“è¯¥ç”¨æˆ·çš„æœ€è¿‘5ä¸ªcommits
                commits_url = f"https://api.github.com/repos/{repo_full_name}/commits"
                params = {'author': username, 'per_page': 5}
                
                try:
                    commits_response = self.session.get(commits_url, headers=headers, params=params, timeout=10)
                    
                    if commits_response.status_code != 200:
                        continue
                    
                    commits = commits_response.json()
                    
                    if not isinstance(commits, list) or not commits:
                        continue
                    
                    # 3. æå–é‚®ç®±
                    for commit in commits:
                        commit_data = commit.get('commit', {})
                        author = commit_data.get('author', {})
                        email = author.get('email', '')
                        
                        if email and 'noreply.github.com' not in email.lower():
                            # æ‰¾åˆ°æœ‰æ•ˆé‚®ç®±ï¼Œç«‹å³è¿”å›
                            return email
                    
                except Exception:
                    continue
            
            return None
            
        except Exception:
            return None
    
    @retry_on_failure(max_retries=3)
    def get_user_repositories(self, username: str, max_repos: int = 30) -> List[Dict]:
        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        from utils.crawler_status import should_stop
        if should_stop():
            logger.debug(f"æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡è·å–ä»“åº“: {username}")
            return []
        
        self._wait()
        
        try:
            url = f"https://github.com/{username}?tab=repositories"
            response = self.session.get(url, headers=self._get_headers(), timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            repositories = []
            
            repo_items = soup.select('div[id="user-repositories-list"] li')
            if not repo_items:
                repo_items = soup.select('li')
            
            for item in repo_items[:max_repos]:
                try:
                    repo_link = item.select_one('a[href*="/"]')
                    if not repo_link:
                        continue
                    
                    href = repo_link.get('href', '')
                    repo_name = href.strip('/')
                    
                    if '/' not in repo_name:
                        repo_name = f"{username}/{repo_name}"
                    
                    desc_elem = item.select_one('p')
                    description = desc_elem.text.strip() if desc_elem else ''
                    
                    stars = 0
                    stars_elem = item.select_one('a[href*="stargazers"]')
                    if stars_elem:
                        try:
                            stars = int(stars_elem.text.strip().replace(',', ''))
                        except:
                            pass
                    
                    lang_elem = item.select_one('[itemprop="programmingLanguage"]')
                    language = lang_elem.text.strip() if lang_elem else ''
                    
                    is_fork = 'fork' in item.text.lower()
                    
                    repositories.append({
                        'repo_id': hash(repo_name),
                        'repo_name': repo_name,
                        'repo_url': f"https://github.com/{repo_name}",
                        'description': description,
                        'stars': stars,
                        'forks': 0,
                        'language': language,
                        'is_fork': is_fork,
                        'created_at': None,
                        'updated_at': None
                    })
                except:
                    continue
            
            logger.info(f"è·å–{username}çš„{len(repositories)}ä¸ªä»“åº“")
            return repositories
            
        except Exception as e:
            logger.error(f"è·å–ä»“åº“å¤±è´¥{username}: {e}")
            return []
    
    @retry_on_failure(max_retries=3)
    def get_repository_contributors(self, repo_full_name: str, max_contributors: int = None) -> tuple[List[Dict], str]:
        """
        è·å–ä»“åº“è´¡çŒ®è€…åˆ—è¡¨ï¼ˆä½¿ç”¨GitHub APIï¼‰
        
        é€šè¿‡GitHubçš„contributors-data APIè·å–å®Œæ•´çš„è´¡çŒ®è€…åˆ—è¡¨
        è¿™æ˜¯æ¨èçš„æ–¹æ³•ï¼Œé€Ÿåº¦å¿«ä¸”ç¨³å®š
        
        Args:
            repo_full_name: ä»“åº“å…¨åï¼Œæ ¼å¼ä¸º "owner/repo"
            max_contributors: æœ€å¤§è·å–æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            
        Returns:
            (contributors, error_msg): è´¡çŒ®è€…åˆ—è¡¨å’Œé”™è¯¯ä¿¡æ¯
            - æˆåŠŸ: ([{"username": "user1", "commits": 100, "rank": 1}, ...], "")
            - å¤±è´¥: ([], "å…·ä½“é”™è¯¯ä¿¡æ¯")
        """
        contributors = []
        owner = repo_full_name.split('/')[0]
        
        try:
            self._wait()
            
            # ä½¿ç”¨GitHubçš„contributors-data API
            api_url = f"https://github.com/{repo_full_name}/graphs/contributors-data"
            
            # éœ€è¦ç‰¹æ®Šçš„headersæ¥è®¿é—®è¿™ä¸ªAPI
            headers = self._get_headers()
            headers['Accept'] = 'application/json'
            headers['X-Requested-With'] = 'XMLHttpRequest'
            
            response = self.session.get(api_url, headers=headers, timeout=15)
            
            # å¤„ç† 202 çŠ¶æ€ç  - GitHub æ­£åœ¨å¼‚æ­¥ç”Ÿæˆæ•°æ®ï¼Œéœ€è¦è½®è¯¢ç­‰å¾…
            if response.status_code == 202:
                logger.info(f"  â³ GitHubæ­£åœ¨ç”Ÿæˆè´¡çŒ®è€…æ•°æ®ï¼Œç­‰å¾…ä¸­...")
                max_retries = 10  # æœ€å¤šç­‰å¾…10æ¬¡
                retry_count = 0
                wait_time = 3  # åˆå§‹ç­‰å¾…3ç§’
                
                while response.status_code == 202 and retry_count < max_retries:
                    retry_count += 1
                    logger.info(f"     ç­‰å¾… {wait_time} ç§’åé‡è¯• ({retry_count}/{max_retries})...")
                    time.sleep(wait_time)
                    
                    # é‡æ–°è¯·æ±‚å‰ä¹Ÿè¦ç­‰å¾…ï¼ˆé¿å…è§¦å‘429ï¼‰
                    self._wait()
                    
                    # é‡æ–°è¯·æ±‚
                    response = self.session.get(api_url, headers=headers, timeout=15)
                    
                    # å¦‚æœè¿˜æ˜¯202ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´ï¼ˆæœ€å¤š10ç§’ï¼‰
                    if response.status_code == 202:
                        wait_time = min(wait_time + 2, 10)
                
                # å¦‚æœè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°è¿˜æ˜¯202
                if response.status_code == 202:
                    return [], f"202 - æ•°æ®ç”Ÿæˆè¶…æ—¶ï¼ˆå·²ç­‰å¾…{retry_count}æ¬¡ï¼Œçº¦{retry_count * 5}ç§’ï¼‰"
                
                # å¦‚æœæˆåŠŸäº†ï¼Œè®°å½•æ—¥å¿—
                if response.status_code == 200:
                    logger.info(f"  âœ“ æ•°æ®å·²å‡†å¤‡å¥½ï¼ˆç­‰å¾…äº†{retry_count}æ¬¡ï¼‰")
            
            # æ£€æŸ¥å…¶ä»–å“åº”çŠ¶æ€
            if response.status_code == 404:
                return [], "404 - ä»“åº“ä¸å­˜åœ¨æˆ–æ— è´¡çŒ®è€…æ•°æ®"
            
            if response.status_code == 429:
                return [], "429 - é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•"
            
            if response.status_code != 200:
                return [], f"{response.status_code} - HTTPé”™è¯¯"
            
            # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
            content_type = response.headers.get('Content-Type', '')
            if 'application/json' not in content_type:
                return [], f"éJSONå“åº” (Content-Type: {content_type})"
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not response.text or response.text.strip() == '':
                return [], "ç©ºå“åº” (çŠ¶æ€ç 200ä½†å†…å®¹ä¸ºç©º)"
            
            # è§£æJSONæ•°æ®
            try:
                data = response.json()
            except ValueError as json_err:
                return [], f"JSONè§£æå¤±è´¥: {str(json_err)}"
            
            if not isinstance(data, list):
                return [], f"æ•°æ®æ ¼å¼é”™è¯¯ (æœŸæœ›listï¼Œå®é™…{type(data).__name__})"
            
            if len(data) == 0:
                return [], "ç©ºåˆ—è¡¨ (ä»“åº“å¯èƒ½æ²¡æœ‰è´¡çŒ®è€…)"
            
            logger.debug(f"APIè¿”å› {len(data)} ä¸ªè´¡çŒ®è€…æ•°æ®")
            
            # GitHub API è¿”å›çš„æ˜¯å‡åºï¼ˆè´¡çŒ®å°‘çš„åœ¨å‰ï¼‰ï¼Œéœ€è¦åè½¬ä¸ºé™åºï¼ˆè´¡çŒ®å¤šçš„åœ¨å‰ï¼‰
            data.reverse()
            logger.debug(f"å·²åè½¬ä¸ºé™åºï¼ˆä¼˜å…ˆå¤„ç†è´¡çŒ®å¤šçš„å¼€å‘è€…ï¼‰")
            
            # æå–ç”¨æˆ·åå’Œè´¡çŒ®åº¦ä¿¡æ¯
            seen_usernames = set()
            filtered_stats = {
                'total': len(data),
                'no_author': 0,
                'no_login': 0,
                'is_owner': 0,
                'duplicate': 0,
                'invalid_format': 0,
                'valid': 0
            }
            
            rank = 0  # æ’åï¼ˆä»1å¼€å§‹ï¼‰
            
            for contributor_data in data:
                # å¦‚æœè®¾ç½®äº†é™åˆ¶ä¸”å·²è¾¾åˆ°ï¼Œåœæ­¢
                if max_contributors and len(contributors) >= max_contributors:
                    break
                
                if not isinstance(contributor_data, dict):
                    continue
                
                author = contributor_data.get('author')
                if not author or not isinstance(author, dict):
                    filtered_stats['no_author'] += 1
                    continue
                
                username = author.get('login')
                if not username:
                    filtered_stats['no_login'] += 1
                    continue
                
                # è¿‡æ»¤ï¼šæ’é™¤ownerã€å»é‡ã€éªŒè¯æ ¼å¼
                if username == owner:
                    filtered_stats['is_owner'] += 1
                    continue
                
                if username in seen_usernames:
                    filtered_stats['duplicate'] += 1
                    continue
                
                if not username.replace('-', '').replace('_', '').isalnum():
                    filtered_stats['invalid_format'] += 1
                    logger.debug(f"è¿‡æ»¤æ— æ•ˆæ ¼å¼ç”¨æˆ·å: {username}")
                    continue
                
                # è·å–è´¡çŒ®åº¦ä¿¡æ¯
                rank += 1
                total_commits = contributor_data.get('total', 0)
                
                contributors.append({
                    'username': username,
                    'commits': total_commits,
                    'rank': rank
                })
                seen_usernames.add(username)
                filtered_stats['valid'] += 1
            
            logger.info(f"  ğŸ“Š è´¡çŒ®è€…ç»Ÿè®¡: APIè¿”å›{filtered_stats['total']}ä¸ª, æœ‰æ•ˆ{filtered_stats['valid']}ä¸ª")
            if filtered_stats['no_author'] > 0 or filtered_stats['no_login'] > 0 or filtered_stats['invalid_format'] > 0:
                logger.info(f"     è¿‡æ»¤: æ— author={filtered_stats['no_author']}, æ— login={filtered_stats['no_login']}, "
                          f"æ˜¯owner={filtered_stats['is_owner']}, æ ¼å¼æ— æ•ˆ={filtered_stats['invalid_format']}")
            
            if not contributors:
                return [], "è¿‡æ»¤åæ— æœ‰æ•ˆè´¡çŒ®è€… (å¯èƒ½éƒ½æ˜¯owneræˆ–æ ¼å¼æ— æ•ˆ)"
            
            return contributors, ""
            
        except requests.exceptions.Timeout:
            return [], "è¯·æ±‚è¶…æ—¶ (15ç§’)"
        except requests.exceptions.ConnectionError:
            return [], "ç½‘ç»œè¿æ¥å¤±è´¥"
        except requests.exceptions.RequestException as req_err:
            return [], f"è¯·æ±‚å¤±è´¥: {str(req_err)}"
        except Exception as e:
            return [], f"æœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}"
    
    def check_is_indie_developer(self, user_info: Dict, repositories: List[Dict]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé€‚åˆWaveSpeedAIåˆä½œçš„ç‹¬ç«‹å¼€å‘è€…
        
        WaveSpeedAIä¸šåŠ¡ï¼šå›¾åƒ/è§†é¢‘ç”ŸæˆAPIå¹³å°
        ç›®æ ‡å®¢æˆ·ï¼šAIåº”ç”¨å¼€å‘è€…ã€å†…å®¹åˆ›ä½œå·¥å…·å¼€å‘è€…ã€APIé›†æˆè€…
        
        åˆ¤æ–­æ ‡å‡†ï¼ˆé’ˆå¯¹ç‹¬ç«‹å¼€å‘è€…ï¼‰ï¼š
        1. ä¸å±äºå¤§å…¬å¸ï¼ˆåŒ…æ‹¬ç«äº‰å¯¹æ‰‹ï¼‰
        2. æœ‰ä¸€å®šå½±å“åŠ›ï¼ˆfollowersæˆ–starsè¾¾åˆ°é…ç½®é˜ˆå€¼ï¼‰
        3. æœ‰æ˜ç¡®çš„AIç›¸å…³é¡¹ç›®ç»éªŒ
        4. **æ ¸å¿ƒï¼šå¿…é¡»æ˜¯ç‹¬ç«‹å¼€å‘è€…ï¼Œä¸æ˜¯æŸä¸ªå¤§é¡¹ç›®çš„æˆå‘˜/è´¡çŒ®è€…**
        """
        # åŠ è½½é…ç½®ï¼ˆä¸€æ¬¡æ€§åŠ è½½ï¼‰
        from utils.config_loader import load_config
        config = load_config()
        github_config = config.get('github', {})
        exclusion_rules = config.get('exclusion_rules', {})
        
        username = user_info.get('username', '')
        
        # 1. æ’é™¤å¤§å…¬å¸å‘˜å·¥ã€ç«äº‰å¯¹æ‰‹å’Œé¡¹ç›®å›¢é˜Ÿæˆå‘˜ï¼ˆä»é…ç½®è¯»å–ï¼‰
        company = (user_info.get('company') or '').lower()
        bio = (user_info.get('bio') or '').lower()
        
        # ä»é…ç½®è¯»å–æ’é™¤çš„ç»„ç»‡åˆ—è¡¨ï¼ˆä»github.exclusion_organizationsè¯»å–ï¼‰
        exclusion_organizations = github_config.get('exclusion_organizations', [])
        
        # è½¬ä¸ºå°å†™åˆ—è¡¨
        exclusion_orgs_lower = [org.lower() for org in exclusion_organizations if org]
        
        for org in exclusion_orgs_lower:
            # æ£€æŸ¥companyå­—æ®µ
            if org in company:
                logger.info(f"âŒ {username} å±äºæ’é™¤çš„ç»„ç»‡: {company}")
                return False
            
            # æ£€æŸ¥bioä¸­æ˜¯å¦æ ‡æ³¨ä¸ºè¯¥ç»„ç»‡çš„æˆå‘˜
            if f"{org} team" in bio or f"{org} member" in bio or f"{org} contributor" in bio:
                logger.info(f"âŒ {username} æ˜¯ {org} ç»„ç»‡æˆå‘˜")
                return False
        
        # 2. è·å–åŸåˆ›ä»“åº“
        original_repos = [r for r in repositories if not r.get('is_fork', False)]
        
        # 3. æ£€æŸ¥å½±å“åŠ›ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–é˜ˆå€¼ï¼‰
        min_followers = github_config.get('min_followers', 100)
        min_stars = github_config.get('min_stars', 500)
        
        followers = user_info.get('followers', 0)
        total_stars = sum(r.get('stars', 0) for r in original_repos)
        
        if followers < min_followers and total_stars < min_stars:
            logger.info(f"âŒ {username} å½±å“åŠ›ä¸è¶³: followers={followers} (éœ€è¦>={min_followers}), stars={total_stars} (éœ€è¦>={min_stars})")
            return False
        
        # 4. æ£€æŸ¥æ˜¯å¦æœ‰çŸ¥åé¡¹ç›®çš„æˆå‘˜æ ‡è¯†ï¼ˆä»é…ç½®è¯»å–ï¼‰
        # é€šè¿‡bioã€companyå­—æ®µæ£€æŸ¥æ˜¯å¦æ ‡æ³¨ä¸ºæŸä¸ªé¡¹ç›®çš„æˆå‘˜
        bio = (user_info.get('bio') or '').lower()
        
        # ä½¿ç”¨åŒæ ·çš„æ’é™¤ç»„ç»‡åˆ—è¡¨
        for project in exclusion_orgs_lower:
            # æ£€æŸ¥æ˜¯å¦åœ¨bioæˆ–companyä¸­æ ‡æ³¨ä¸ºè¯¥é¡¹ç›®æˆå‘˜
            if f"{project} team" in bio or f"{project} member" in bio or f"{project} contributor" in bio:
                logger.info(f"âŒ {username} æ˜¯ {project} é¡¹ç›®æˆå‘˜")
                return False
            if project in company and ('team' in company or 'member' in company):
                logger.info(f"âŒ {username} åœ¨companyä¸­æ ‡æ³¨ä¸º {project} æˆå‘˜")
                return False
        
        # 5. æ£€æŸ¥æ˜¯å¦æœ‰AIç›¸å…³é¡¹ç›®ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–å…³é”®è¯ï¼‰
        core_ai_keywords = github_config.get('core_ai_keywords', [
            # é»˜è®¤å…³é”®è¯ï¼ˆå¦‚æœé…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ï¼‰
            'machine-learning', 'deep-learning', 'neural-network', 'ml-model',
            'pytorch', 'tensorflow', 'keras', 'scikit-learn',
            'stable-diffusion', 'diffusion-model', 'text-to-image', 'text-to-video',
            'image-generation', 'video-generation', 'generative-ai', 'gan',
            'controlnet', 'animatediff',
            'gpt', 'llm', 'large-language-model', 'chatbot', 'transformer',
            'bert', 'nlp', 'natural-language',
            'computer-vision', 'object-detection', 'image-recognition',
            'yolo', 'opencv-ai', 'face-recognition'
        ])
        
        has_ai_project = False
        ai_repo_name = None
        
        for repo in original_repos:
            repo_name = repo.get('repo_name', '').lower()
            description = repo.get('description', '').lower()
            language = repo.get('language', '').lower()
            
            repo_text = f"{repo_name} {description} {language}"
            
            # æ£€æŸ¥æ ¸å¿ƒAIå…³é”®è¯ï¼ˆä»»æ„ä¸€ä¸ªå³å¯ï¼‰
            matched_core = [kw for kw in core_ai_keywords if kw in repo_text]
            
            if matched_core:
                has_ai_project = True
                ai_repo_name = repo.get('repo_name')
                logger.info(f"âœ“ {username} æœ‰AIé¡¹ç›®: {ai_repo_name}")
                logger.info(f"  åŒ¹é…å…³é”®è¯: {matched_core}")
                break
        
        if not has_ai_project:
            logger.info(f"âŒ {username} æ²¡æœ‰æ˜ç¡®çš„AIç›¸å…³é¡¹ç›®")
            return False
        
        logger.info(f"âœ“ {username} åˆæ ¼ - ç‹¬ç«‹å¼€å‘è€… - Followers:{followers}, Stars:{total_stars}")
        return True
    
    def check_is_academic(self, user_info: Dict, repositories: List[Dict]) -> tuple[bool, list, list]:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå­¦æœ¯äººå£«
        
        å­¦æœ¯äººå£«ç‰¹å¾ï¼š
        1. Bio/CompanyåŒ…å«å­¦æœ¯æœºæ„å…³é”®è¯
        2. é¡¹ç›®ä¸»è¦æ˜¯ç ”ç©¶æ€§è´¨ï¼ˆè®ºæ–‡å¤ç°ã€æ¨¡å‹è®­ç»ƒã€ç ”ç©¶å·¥å…·ï¼‰
        3. æ·±åº¦å­¦ä¹ /ç¥ç»ç½‘ç»œç›¸å…³çš„ç ”ç©¶é¡¹ç›®
        4. æ»¡è¶³å½±å“åŠ›è¦æ±‚ï¼ˆfollowersæˆ–starsè¾¾åˆ°é…ç½®é˜ˆå€¼ï¼‰
        
        Args:
            user_info: ç”¨æˆ·ä¿¡æ¯
            repositories: ä»“åº“åˆ—è¡¨
            
        Returns:
            (is_academic, academic_indicators, research_areas)
            - is_academic: æ˜¯å¦ä¸ºå­¦æœ¯äººå£«
            - academic_indicators: å­¦æœ¯æŒ‡æ ‡åˆ—è¡¨
            - research_areas: ç ”ç©¶é¢†åŸŸåˆ—è¡¨
        """
        from utils.config_loader import load_config
        config = load_config()
        github_config = config.get('github', {})
        
        username = user_info.get('username', '')
        bio = (user_info.get('bio') or '').lower()
        company = (user_info.get('company') or '').lower()
        location = (user_info.get('location') or '').lower()
        
        academic_indicators = []
        research_areas = []
        
        # 1. ä»é…ç½®æ–‡ä»¶è¯»å–å­¦æœ¯æœºæ„å…³é”®è¯
        academic_keywords = github_config.get('academic_keywords', [
            'university', 'college', 'institute', 'research', 'lab', 'laboratory',
            'phd', 'ph.d', 'professor', 'postdoc', 'post-doc', 'student',
            'academic', 'scholar', 'researcher', 'faculty',
            'å¤§å­¦', 'å­¦é™¢', 'ç ”ç©¶æ‰€', 'å®éªŒå®¤', 'åšå£«', 'æ•™æˆ', 'ç ”ç©¶å‘˜', 'å­¦è€…'
        ])
        
        for keyword in academic_keywords:
            if keyword.lower() in bio or keyword.lower() in company or keyword.lower() in location:
                academic_indicators.append(f"Profile contains: {keyword}")
                break
        
        # 2. æ£€æŸ¥é¡¹ç›®ç‰¹å¾
        research_keywords = {
            'deep-learning': ['deep-learning', 'deep learning', 'deeplearning', 'dl'],
            'neural-network': ['neural-network', 'neural network', 'neuralnetwork', 'nn'],
            'machine-learning': ['machine-learning', 'machine learning', 'machinelearning', 'ml'],
            'computer-vision': ['computer-vision', 'computer vision', 'cv', 'image-processing'],
            'nlp': ['nlp', 'natural-language', 'natural language processing', 'text-processing'],
            'reinforcement-learning': ['reinforcement-learning', 'reinforcement learning', 'rl'],
            'multimodal': ['multimodal', 'multi-modal', 'vision-language', 'vlm'],
            'transformer': ['transformer', 'attention', 'bert', 'gpt'],
            'diffusion': ['diffusion', 'stable-diffusion', 'ddpm', 'ddim'],
            'gan': ['gan', 'generative adversarial', 'stylegan'],
        }
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–ç ”ç©¶é¡¹ç›®å…³é”®è¯
        research_project_indicators = github_config.get('research_project_keywords', [
            'paper', 'arxiv', 'implementation', 'reproduction', 'reproduce',
            'research', 'experiment', 'benchmark', 'dataset', 'pretrained',
            'model', 'training', 'pytorch', 'tensorflow', 'keras',
            'è®ºæ–‡', 'å¤ç°', 'å®éªŒ', 'ç ”ç©¶'
        ])
        
        # ç»Ÿè®¡ç ”ç©¶é¡¹ç›®æ•°é‡
        research_project_count = 0
        matched_areas = set()
        
        for repo in repositories:
            repo_name = repo.get('repo_name', '').lower()
            description = repo.get('description', '').lower()
            repo_text = f"{repo_name} {description}"
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç ”ç©¶é¡¹ç›®
            is_research = any(indicator.lower() in repo_text for indicator in research_project_indicators)
            
            if is_research:
                research_project_count += 1
                
                # è¯†åˆ«ç ”ç©¶é¢†åŸŸ
                for area, keywords in research_keywords.items():
                    if any(kw in repo_text for kw in keywords):
                        matched_areas.add(area)
        
        if research_project_count > 0:
            academic_indicators.append(f"Research projects: {research_project_count}")
        
        if matched_areas:
            research_areas = list(matched_areas)
            academic_indicators.append(f"Research areas: {', '.join(research_areas)}")
        
        # 3. åˆ¤æ–­æ˜¯å¦ä¸ºå­¦æœ¯äººå£«
        # æ¡ä»¶ï¼šæœ‰å­¦æœ¯æœºæ„å…³é”®è¯ æˆ– æœ‰2ä¸ªä»¥ä¸Šç ”ç©¶é¡¹ç›®
        is_academic = len(academic_indicators) > 0 and (
            any('Profile contains' in ind for ind in academic_indicators) or
            research_project_count >= 2
        )
        
        if not is_academic:
            return False, [], []
        
        # 4. æ£€æŸ¥å½±å“åŠ›ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–å­¦æœ¯äººå£«çš„é˜ˆå€¼ï¼‰
        academic_min_followers = github_config.get('academic_min_followers', 50)
        academic_min_stars = github_config.get('academic_min_stars', 100)
        
        followers = user_info.get('followers', 0)
        original_repos = [r for r in repositories if not r.get('is_fork', False)]
        total_stars = sum(r.get('stars', 0) for r in original_repos)
        
        if followers < academic_min_followers and total_stars < academic_min_stars:
            logger.info(f"âŒ {username} å­¦æœ¯äººå£«å½±å“åŠ›ä¸è¶³: followers={followers} (éœ€è¦>={academic_min_followers}), stars={total_stars} (éœ€è¦>={academic_min_stars})")
            return False, [], []
        
        logger.info(f"âœ“ {username} è¯†åˆ«ä¸ºå­¦æœ¯äººå£«")
        logger.info(f"  å­¦æœ¯æŒ‡æ ‡: {academic_indicators}")
        logger.info(f"  ç ”ç©¶é¢†åŸŸ: {research_areas}")
        logger.info(f"  å½±å“åŠ›: Followers={followers}, Stars={total_stars}")
        
        return is_academic, academic_indicators, research_areas
